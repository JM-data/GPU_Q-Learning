{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import timeit\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import keras\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "from keras.models import model_from_json\n",
    "import time\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Definition of the Neural Network with Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    X[np.where(X < 0)] = 0\n",
    "    return(X)\n",
    "\n",
    "class output_layer(object):\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \n",
    " \n",
    "        self.W = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        self.y_pred = T.dot(input, self.W) + self.b\n",
    "\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "    def mse(self, y):\n",
    "        return T.mean((self.y_pred - y) ** 2) \n",
    "    \n",
    "class HiddenLayer(object):\n",
    "    def __init__(self, input, n_in, n_out, W=None, b=None):\n",
    "\n",
    "        self.input = input\n",
    "        if W is None:\n",
    "            W_values = np.asarray(\n",
    "                np.random.uniform(low=-0.1,high=0.1,size=(n_in,  n_out)),\n",
    "                dtype=theano.config.floatX)\n",
    "            W_h = theano.shared(value = W_values, name='W_h', borrow=True)\n",
    "            \n",
    "        if b is None:\n",
    "            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b_h = theano.shared(value=b_values, name='b_h', borrow=True)\n",
    "\n",
    "        self.W_h = W_h\n",
    "        self.b_h = b_h\n",
    "        \n",
    "        self.params = [self.W_h, self.b_h] # Parameters of the model\n",
    "        \n",
    "        self.output = T.nnet.relu(T.dot(input, self.W_h) + self.b_h)\n",
    "\n",
    "        self.input = input\n",
    "    \n",
    "class Neural_Network(object):\n",
    "    \n",
    "    def __init__(self, input, n_in, n_hidden, n_out):\n",
    "\n",
    "        self.hiddenLayer = HiddenLayer(\n",
    "            input=input,\n",
    "            n_in= n_in,\n",
    "            n_out=n_hidden,\n",
    "        )\n",
    "\n",
    "        self.output_layer = output_layer(\n",
    "            input=self.hiddenLayer.output,\n",
    "            n_in=n_hidden,\n",
    "            n_out=n_out\n",
    "        )\n",
    "\n",
    "\n",
    "        self.mse = (\n",
    "            self.output_layer.mse\n",
    "        )\n",
    "\n",
    "        self.params = self.hiddenLayer.params + self.output_layer.params\n",
    "        \n",
    "        self.input = input\n",
    "        \n",
    "def train_on_batch(dataset_X, dataset_y, classifier, x,y,index,learning_rate=0.01, batch_size=1, n_hidden=500):\n",
    "\n",
    "    train_set_x = theano.shared(np.asarray(dataset_X,\n",
    "                                            dtype=theano.config.floatX),\n",
    "                                borrow=True)\n",
    "    train_set_y = theano.shared(np.asarray(dataset_y,\n",
    "                                            dtype=theano.config.floatX),\n",
    "                                borrow=True)\n",
    "\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "\n",
    "\n",
    "    cost = (\n",
    "        classifier.mse(y)\n",
    "    )\n",
    "\n",
    "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "    \n",
    "    updates = [\n",
    "        (param, param - learning_rate * gparam)\n",
    "        for param, gparam in zip(classifier.params, gparams)\n",
    "    ]\n",
    "\n",
    "\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "  \n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    error = []\n",
    "    error_min = np.inf\n",
    "    for minibatch_index in range(n_train_batches):\n",
    "        minibatch_avg_cost = train_model(minibatch_index)\n",
    "        linear_1 = np.dot(dataset_X, classifier.hiddenLayer.W_h.eval()) +  classifier.hiddenLayer.b_h.eval()\n",
    "        h1 = relu(linear_1)\n",
    "        output_nn = np.dot(h1,classifier.output_layer.W.eval()) + classifier.output_layer.b.eval()\n",
    "        error.append(np.mean((output_nn - dataset_y)**2))\n",
    "        \n",
    "        '''if error[len(error)-1] < error_min:\n",
    "            error_min = error[len(error)-1]\n",
    "            best_W1 = classifier.hiddenLayer.W_h.eval()\n",
    "            best_b1 = classifier.hiddenLayer.b_h.eval()\n",
    "            best_Wo = classifier.output_layer.W.eval()\n",
    "            best_bo = classifier.output_layer.b.eval()'''\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    \n",
    "    return(error, classifier.hiddenLayer.W_h.eval(), classifier.hiddenLayer.b_h.eval(), classifier.output_layer.W.eval(), classifier.output_layer.b.eval())\n",
    "\n",
    "def predict_NN(X_input, W1, b1, Wo, bo):\n",
    "    return(np.dot(relu(np.dot(X_input,W1) + b1),Wo) + bo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        \n",
    "        \"\"\"Define max length of memory and gamma\"\"\"\n",
    "         \n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "\n",
    "        \"\"\"Add experience to memory\"\"\"\n",
    "        \n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, W1, b1, Wo, bo, batch_size=10):\n",
    "        \n",
    "        \"\"\"Get the batch input and targets we will train on\"\"\"\n",
    "        \n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = 2\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        \n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        \n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,\n",
    "                                                  size=inputs.shape[0])):\n",
    "            \n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "            inputs[i:i+1] = state_t\n",
    "            targets[i] = predict_NN(state_t, W1, b1, Wo, bo)\n",
    "            Q_sa = np.max(targets[i])\n",
    "            if game_over:\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CartPole on OpenAI Gym and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters : epsilon : 0.1 C : 50 , learning rate : 0.1 batch size for training : 50\n",
      "Training for  2000 time-steps ...\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "learning_rate=0.1\n",
    "epsilon = .1\n",
    "num_actions = env.action_space.n \n",
    "max_memory = 4000000000\n",
    "hidden_size = 200\n",
    "batch_size = 50\n",
    "acc_reward=0\n",
    "observation_shape = env.observation_space.shape[0]\n",
    "time_step=0\n",
    "max_time_steps=2000\n",
    "everyC=50\n",
    "exp_replay = ExperienceReplay(max_memory=max_memory)\n",
    "win_cnt = 0\n",
    "t0 = time.time()\n",
    "actual_total=0\n",
    "e=0\n",
    "\n",
    "print('Parameters :','epsilon :', epsilon,'C :', everyC,', learning rate :', learning_rate, 'batch size for training :', batch_size)\n",
    "\n",
    "print('Training for ',max_time_steps,'time-steps ...')\n",
    "N = 6\n",
    "X = np.random.uniform(low=-5.,high=5.,size=(N, 4)).astype('float32')\n",
    "W = np.random.uniform(low=-5.,high=5.,size=(4, 2)).astype('float32')\n",
    "b = np.random.uniform(low=-5.,high=5.,size=2).astype('float32') \n",
    "\n",
    "noise = np.random.normal(0,1,(N,2))\n",
    "\n",
    "y = np.dot(X**2,W) + 5*np.dot(X,W) +  b + noise\n",
    "y=y.astype('float32')\n",
    "\n",
    "\n",
    "# allocate symbolic variables for the data\n",
    "index = T.lscalar()  \n",
    "x = T.matrix('x') \n",
    "y_g = T.matrix('y')  \n",
    "# construct the neural net\n",
    "classifier = Neural_Network(input=x,n_in=4,n_hidden=200,n_out=2)\n",
    "\n",
    "#initalize with a random backpropagation\n",
    "err, W1, b1, Wo, bo = train_on_batch(X, y, classifier, x=x,y=y_g,index=index, learning_rate=0.0003, batch_size=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 time steps done,  1 episodes done. Reward : 10.0 , loss : 0.4012257924446841\n",
      "18 time steps done,  2 episodes done. Reward : 8.0 , loss : 0.5729455781534732\n",
      "28 time steps done,  3 episodes done. Reward : 10.0 , loss : 0.5932333386763372\n",
      "38 time steps done,  4 episodes done. Reward : 10.0 , loss : 0.9425078980307213\n",
      "49 time steps done,  5 episodes done. Reward : 11.0 , loss : 1.1837238921061104\n",
      "58 time steps done,  6 episodes done. Reward : 9.0 , loss : 1.0481504387440348\n",
      "66 time steps done,  7 episodes done. Reward : 8.0 , loss : 0.659323204597631\n",
      "76 time steps done,  8 episodes done. Reward : 10.0 , loss : 0.7448139303352894\n",
      "84 time steps done,  9 episodes done. Reward : 8.0 , loss : 0.5527741857669465\n",
      "93 time steps done,  10 episodes done. Reward : 9.0 , loss : 0.6443421888676423\n",
      "104 time steps done,  11 episodes done. Reward : 11.0 , loss : 1.0688422354038702\n",
      "113 time steps done,  12 episodes done. Reward : 9.0 , loss : 0.6668446120723764\n",
      "123 time steps done,  13 episodes done. Reward : 10.0 , loss : 0.45239448625151363\n",
      "135 time steps done,  14 episodes done. Reward : 12.0 , loss : 0.2145905161801185\n",
      "147 time steps done,  15 episodes done. Reward : 12.0 , loss : 0.4260970292016248\n",
      "157 time steps done,  16 episodes done. Reward : 10.0 , loss : 0.40683808818998285\n",
      "168 time steps done,  17 episodes done. Reward : 11.0 , loss : 0.21365864688677927\n",
      "179 time steps done,  18 episodes done. Reward : 11.0 , loss : 0.3570840154421221\n",
      "189 time steps done,  19 episodes done. Reward : 10.0 , loss : 0.7733475860995896\n",
      "199 time steps done,  20 episodes done. Reward : 10.0 , loss : 5.88534267748743\n",
      "209 time steps done,  21 episodes done. Reward : 10.0 , loss : 0.8898923068837328\n",
      "220 time steps done,  22 episodes done. Reward : 11.0 , loss : 0.30348915379029573\n",
      "232 time steps done,  23 episodes done. Reward : 12.0 , loss : 0.28270034100695396\n",
      "241 time steps done,  24 episodes done. Reward : 9.0 , loss : 0.7869135301720708\n",
      "251 time steps done,  25 episodes done. Reward : 10.0 , loss : 0.3354111858631532\n",
      "259 time steps done,  26 episodes done. Reward : 8.0 , loss : 0.19459356468369815\n",
      "268 time steps done,  27 episodes done. Reward : 9.0 , loss : 0.30212828681568427\n",
      "278 time steps done,  28 episodes done. Reward : 10.0 , loss : 0.357017979691046\n",
      "288 time steps done,  29 episodes done. Reward : 10.0 , loss : 0.25312657445722464\n",
      "300 time steps done,  30 episodes done. Reward : 12.0 , loss : 0.22395856344383017\n",
      "309 time steps done,  31 episodes done. Reward : 9.0 , loss : 0.17171710270802257\n",
      "318 time steps done,  32 episodes done. Reward : 9.0 , loss : 0.7439016646656479\n",
      "328 time steps done,  33 episodes done. Reward : 10.0 , loss : 0.30086686093362924\n",
      "338 time steps done,  34 episodes done. Reward : 10.0 , loss : 0.18203201829822596\n",
      "348 time steps done,  35 episodes done. Reward : 10.0 , loss : 0.6101711241199592\n",
      "357 time steps done,  36 episodes done. Reward : 9.0 , loss : 0.2952895855799879\n",
      "367 time steps done,  37 episodes done. Reward : 10.0 , loss : 2.316590328309219\n",
      "377 time steps done,  38 episodes done. Reward : 10.0 , loss : 0.23847186442902393\n",
      "385 time steps done,  39 episodes done. Reward : 8.0 , loss : 0.3860467082467531\n",
      "394 time steps done,  40 episodes done. Reward : 9.0 , loss : 0.29277133434466757\n",
      "402 time steps done,  41 episodes done. Reward : 8.0 , loss : 0.329625065761889\n",
      "412 time steps done,  42 episodes done. Reward : 10.0 , loss : 0.24954842556707438\n",
      "422 time steps done,  43 episodes done. Reward : 10.0 , loss : 0.5009585730479696\n",
      "434 time steps done,  44 episodes done. Reward : 12.0 , loss : 0.27115886216362545\n",
      "444 time steps done,  45 episodes done. Reward : 10.0 , loss : 0.21718637844436914\n",
      "455 time steps done,  46 episodes done. Reward : 11.0 , loss : 0.4365239589127457\n",
      "465 time steps done,  47 episodes done. Reward : 10.0 , loss : 0.6493480104205065\n",
      "475 time steps done,  48 episodes done. Reward : 10.0 , loss : 0.34804561915255194\n",
      "483 time steps done,  49 episodes done. Reward : 8.0 , loss : 0.9737081632855771\n",
      "491 time steps done,  50 episodes done. Reward : 8.0 , loss : 0.5559754017596612\n",
      "500 time steps done,  51 episodes done. Reward : 9.0 , loss : 0.4470962671551098\n",
      "509 time steps done,  52 episodes done. Reward : 9.0 , loss : 2.7850514095774606\n",
      "520 time steps done,  53 episodes done. Reward : 11.0 , loss : 1.3928401943083022\n",
      "529 time steps done,  54 episodes done. Reward : 9.0 , loss : 0.7811414083487247\n",
      "539 time steps done,  55 episodes done. Reward : 10.0 , loss : 0.4552272256670318\n",
      "550 time steps done,  56 episodes done. Reward : 11.0 , loss : 3.735983686175361\n",
      "560 time steps done,  57 episodes done. Reward : 10.0 , loss : 0.13064395809685028\n",
      "569 time steps done,  58 episodes done. Reward : 9.0 , loss : 0.6728045090382715\n",
      "579 time steps done,  59 episodes done. Reward : 10.0 , loss : 0.35842466751663515\n",
      "590 time steps done,  60 episodes done. Reward : 11.0 , loss : 0.7393119720566871\n",
      "602 time steps done,  61 episodes done. Reward : 12.0 , loss : 0.38290219912575324\n",
      "611 time steps done,  62 episodes done. Reward : 9.0 , loss : 0.2822118566060477\n",
      "621 time steps done,  63 episodes done. Reward : 10.0 , loss : 0.31679241351110055\n",
      "633 time steps done,  64 episodes done. Reward : 12.0 , loss : 0.32969625141087006\n",
      "645 time steps done,  65 episodes done. Reward : 12.0 , loss : 0.4880865418201203\n",
      "656 time steps done,  66 episodes done. Reward : 11.0 , loss : 0.714083445220152\n",
      "665 time steps done,  67 episodes done. Reward : 9.0 , loss : 0.29989773051286744\n",
      "675 time steps done,  68 episodes done. Reward : 10.0 , loss : 0.24867642269033907\n",
      "684 time steps done,  69 episodes done. Reward : 9.0 , loss : 0.24147107271395454\n",
      "693 time steps done,  70 episodes done. Reward : 9.0 , loss : 0.17312478459304842\n",
      "702 time steps done,  71 episodes done. Reward : 9.0 , loss : 0.2509966583718377\n",
      "711 time steps done,  72 episodes done. Reward : 9.0 , loss : 1.0710422595477391\n",
      "723 time steps done,  73 episodes done. Reward : 12.0 , loss : 2.6903189437787876\n",
      "747 time steps done,  74 episodes done. Reward : 24.0 , loss : 0.11090473150850531\n",
      "768 time steps done,  75 episodes done. Reward : 21.0 , loss : 0.4894739126329856\n",
      "782 time steps done,  76 episodes done. Reward : 14.0 , loss : 0.3094725057436005\n",
      "792 time steps done,  77 episodes done. Reward : 10.0 , loss : 1.9549562340358575\n",
      "802 time steps done,  78 episodes done. Reward : 10.0 , loss : 0.1433736018320429\n",
      "812 time steps done,  79 episodes done. Reward : 10.0 , loss : 0.5852806968013177\n",
      "823 time steps done,  80 episodes done. Reward : 11.0 , loss : 1.2178682618665286\n",
      "832 time steps done,  81 episodes done. Reward : 9.0 , loss : 0.5405544119923203\n",
      "918 time steps done,  82 episodes done. Reward : 86.0 , loss : 0.1647240475524322\n",
      "958 time steps done,  83 episodes done. Reward : 40.0 , loss : 0.21878929651656256\n",
      "975 time steps done,  84 episodes done. Reward : 17.0 , loss : 0.29883050872489253\n",
      "990 time steps done,  85 episodes done. Reward : 15.0 , loss : 2.0555365385517037\n",
      "1007 time steps done,  86 episodes done. Reward : 17.0 , loss : 0.5122067603412302\n",
      "1046 time steps done,  87 episodes done. Reward : 39.0 , loss : 0.17889118677278845\n",
      "1062 time steps done,  88 episodes done. Reward : 16.0 , loss : 0.1808253693482155\n",
      "1077 time steps done,  89 episodes done. Reward : 15.0 , loss : 0.36083235484039605\n",
      "1154 time steps done,  90 episodes done. Reward : 77.0 , loss : 0.23625937991196277\n",
      "1179 time steps done,  91 episodes done. Reward : 25.0 , loss : 0.25568549089641285\n",
      "1210 time steps done,  92 episodes done. Reward : 31.0 , loss : 3.059066856075707\n",
      "1231 time steps done,  93 episodes done. Reward : 21.0 , loss : 0.20104130338617543\n",
      "1249 time steps done,  94 episodes done. Reward : 18.0 , loss : 2.2464558416160565\n",
      "1259 time steps done,  95 episodes done. Reward : 10.0 , loss : 0.27266154077551513\n",
      "1278 time steps done,  96 episodes done. Reward : 19.0 , loss : 0.1272960851240501\n",
      "1304 time steps done,  97 episodes done. Reward : 26.0 , loss : 0.8819589267592062\n",
      "1313 time steps done,  98 episodes done. Reward : 9.0 , loss : 0.3089808627986648\n",
      "1323 time steps done,  99 episodes done. Reward : 10.0 , loss : 0.5006739491054283\n",
      "1334 time steps done,  100 episodes done. Reward : 11.0 , loss : 0.3758912892251982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1343 time steps done,  101 episodes done. Reward : 9.0 , loss : 0.15978445998465096\n",
      "1354 time steps done,  102 episodes done. Reward : 11.0 , loss : 0.15233961273833696\n",
      "1382 time steps done,  103 episodes done. Reward : 28.0 , loss : 0.11731342088278089\n",
      "1397 time steps done,  104 episodes done. Reward : 15.0 , loss : 0.21231766407781946\n",
      "1422 time steps done,  105 episodes done. Reward : 25.0 , loss : 0.43454560671561\n",
      "1440 time steps done,  106 episodes done. Reward : 18.0 , loss : 0.304129751422211\n",
      "1451 time steps done,  107 episodes done. Reward : 11.0 , loss : 1.354383074393273\n",
      "1474 time steps done,  108 episodes done. Reward : 23.0 , loss : 3.485006194801568\n",
      "1491 time steps done,  109 episodes done. Reward : 17.0 , loss : 0.11202781870221105\n",
      "1522 time steps done,  110 episodes done. Reward : 31.0 , loss : 0.22982442397148972\n",
      "1534 time steps done,  111 episodes done. Reward : 12.0 , loss : 0.7025360164272869\n",
      "1561 time steps done,  112 episodes done. Reward : 27.0 , loss : 0.33179781581344076\n",
      "1592 time steps done,  113 episodes done. Reward : 31.0 , loss : 0.2546094867872642\n",
      "1611 time steps done,  114 episodes done. Reward : 19.0 , loss : 0.31355859711636247\n",
      "1641 time steps done,  115 episodes done. Reward : 30.0 , loss : 0.2599597808739443\n",
      "1679 time steps done,  116 episodes done. Reward : 38.0 , loss : 0.08934816556525987\n",
      "1699 time steps done,  117 episodes done. Reward : 20.0 , loss : 1.2809484784276066\n",
      "1732 time steps done,  118 episodes done. Reward : 33.0 , loss : 0.9886342639128373\n",
      "1742 time steps done,  119 episodes done. Reward : 10.0 , loss : 0.23798397702361826\n",
      "1763 time steps done,  120 episodes done. Reward : 21.0 , loss : 0.12035781106978741\n",
      "1783 time steps done,  121 episodes done. Reward : 20.0 , loss : 0.5011450495256523\n",
      "1810 time steps done,  122 episodes done. Reward : 27.0 , loss : 0.240722096858794\n",
      "1830 time steps done,  123 episodes done. Reward : 20.0 , loss : 0.10481058129450721\n",
      "1859 time steps done,  124 episodes done. Reward : 29.0 , loss : 0.4136144003767537\n",
      "1867 time steps done,  125 episodes done. Reward : 8.0 , loss : 0.20146022680512624\n",
      "1886 time steps done,  126 episodes done. Reward : 19.0 , loss : 0.10079830499203782\n",
      "1899 time steps done,  127 episodes done. Reward : 13.0 , loss : 0.22202574051905835\n",
      "1919 time steps done,  128 episodes done. Reward : 20.0 , loss : 0.08797786286992122\n",
      "1933 time steps done,  129 episodes done. Reward : 14.0 , loss : 1.389161528889918\n",
      "1963 time steps done,  130 episodes done. Reward : 30.0 , loss : 0.23956877664803147\n",
      "1978 time steps done,  131 episodes done. Reward : 15.0 , loss : 0.0784653127795736\n",
      "1992 time steps done,  132 episodes done. Reward : 14.0 , loss : 3.780649086038758\n",
      "2011 time steps done,  133 episodes done. Reward : 19.0 , loss : 0.11056124632190346\n",
      "Total training time : 661.5473699569702 Actual training time : 658.0582566261292\n",
      "Win ratio (nb of games won/nb of games played) : 0.0\n"
     ]
    }
   ],
   "source": [
    "while time_step<max_time_steps:\n",
    "    loss = 0.\n",
    "    acc_reward = 0\n",
    "    C=0\n",
    "    e+=1\n",
    "    input_t = env.reset()\n",
    "    input_t = input_t.reshape((1,observation_shape))\n",
    "    game_over = False\n",
    "    \n",
    "    while not game_over:\n",
    "\n",
    "        input_tm1 = input_t.astype('float32')\n",
    "        \n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(0, num_actions, size=1)[0]\n",
    "        else:\n",
    "            q = predict_NN(input_tm1, W1, b1, Wo, bo)\n",
    "            action = np.argmax(q)\n",
    "\n",
    "        input_t, reward, game_over, infodemerde = env.step(action)\n",
    "        input_t = input_t.reshape((1,observation_shape))\n",
    "        \n",
    "        acc_reward += reward\n",
    "\n",
    "        exp_replay.remember([input_tm1, action, reward, input_t], game_over)\n",
    "\n",
    "        inputs, targets = exp_replay.get_batch(W1, b1, Wo, bo, batch_size=batch_size)\n",
    "        inputs=inputs.astype('float32')\n",
    "        targets=targets.astype('float32')\n",
    "\n",
    "        t2 = time.time()  #start of actual training time\n",
    "\n",
    "        #TRAIN\n",
    "        err, W1, b1, Wo, bo = train_on_batch(inputs, targets, classifier, x=x, y=y_g, index=index, learning_rate=learning_rate, batch_size=len(inputs))\n",
    "\n",
    "        t3 = time.time() #end of actual training time\n",
    "        actual_total += t3-t2\n",
    "\n",
    "        time_step += 1\n",
    "\n",
    "        if acc_reward>=200:\n",
    "            game_over=True\n",
    "            win_cnt+=1\n",
    "\n",
    "    print(time_step,'time steps done, ',e,'episodes done. Reward :', acc_reward, ', loss :', err[0])\n",
    "\n",
    "t1 = time.time() #end of training time\n",
    "total = t1-t0\n",
    "print('Total training time :', total,'Actual training time :', actual_total)\n",
    "print('Win ratio (nb of games won/nb of games played) :', win_cnt/e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for  10 episodes ...\n",
      "0 episodes done. Reward : 17.0\n",
      "1 episodes done. Reward : 15.0\n",
      "2 episodes done. Reward : 12.0\n",
      "3 episodes done. Reward : 19.0\n",
      "4 episodes done. Reward : 17.0\n",
      "5 episodes done. Reward : 15.0\n",
      "6 episodes done. Reward : 15.0\n",
      "7 episodes done. Reward : 13.0\n",
      "8 episodes done. Reward : 15.0\n",
      "9 episodes done. Reward : 13.0\n",
      "The average reward over the test was : 15.1\n"
     ]
    }
   ],
   "source": [
    "#nb of episodes to test\n",
    "nb_e_test=10\n",
    "\n",
    "#Total reward over the episodes\n",
    "total_rew=0\n",
    "\n",
    "print('Testing for ',nb_e_test,'episodes ...')\n",
    "\n",
    "for episode in range(nb_e_test):\n",
    "    acc_reward = 0\n",
    "    input_t = env.reset()\n",
    "    input_t = input_t.reshape((1,observation_shape))\n",
    "    game_over = False\n",
    "\n",
    "    while not game_over:\n",
    "        input_tm1 = input_t.astype('float32')\n",
    "        \n",
    "        q = predict_NN(input_tm1, W1, b1, Wo, bo)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        input_t, reward, game_over, infodemerde = env.step(action)\n",
    "        input_t = input_t.reshape((1,observation_shape))\n",
    "        \n",
    "        acc_reward += reward\n",
    "\n",
    "        if acc_reward>=200:\n",
    "            game_over=True\n",
    "            \n",
    "    total_rew+=acc_reward\n",
    "\n",
    "    print(episode,'episodes done. Reward :', acc_reward)\n",
    "\n",
    "\n",
    "print('The average reward over the test was :',total_rew/nb_e_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
