{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import timeit\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import keras\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "from keras.models import model_from_json\n",
    "import time\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Definition of the Neural Network with Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    X[np.where(X < 0)] = 0\n",
    "    return(X)\n",
    "\n",
    "class output_layer(object):\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \n",
    " \n",
    "        self.W = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        self.y_pred = T.dot(input, self.W) + self.b\n",
    "\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "    def mse(self, y):\n",
    "        return T.mean((self.y_pred - y) ** 2) \n",
    "    \n",
    "class HiddenLayer(object):\n",
    "    def __init__(self, input, n_in, n_out, W=None, b=None):\n",
    "\n",
    "        self.input = input\n",
    "        if W is None:\n",
    "            W_values = np.asarray(\n",
    "                np.random.uniform(low=-0.1,high=0.1,size=(n_in,  n_out)),\n",
    "                dtype=theano.config.floatX)\n",
    "            W_h = theano.shared(value = W_values, name='W_h', borrow=True)\n",
    "            \n",
    "        if b is None:\n",
    "            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b_h = theano.shared(value=b_values, name='b_h', borrow=True)\n",
    "\n",
    "        self.W_h = W_h\n",
    "        self.b_h = b_h\n",
    "        \n",
    "        self.params = [self.W_h, self.b_h] # Parameters of the model\n",
    "        \n",
    "        self.output = T.nnet.relu(T.dot(input, self.W_h) + self.b_h)\n",
    "\n",
    "        self.input = input\n",
    "    \n",
    "class Neural_Network(object):\n",
    "    \n",
    "    def __init__(self, input, n_in, n_hidden, n_out):\n",
    "\n",
    "        self.hiddenLayer = HiddenLayer(\n",
    "            input=input,\n",
    "            n_in= n_in,\n",
    "            n_out=n_hidden,\n",
    "        )\n",
    "\n",
    "        self.output_layer = output_layer(\n",
    "            input=self.hiddenLayer.output,\n",
    "            n_in=n_hidden,\n",
    "            n_out=n_out\n",
    "        )\n",
    "\n",
    "\n",
    "        self.mse = (\n",
    "            self.output_layer.mse\n",
    "        )\n",
    "\n",
    "        self.params = self.hiddenLayer.params + self.output_layer.params\n",
    "        \n",
    "        self.input = input\n",
    "        \n",
    "def train_on_batch(dataset_X, dataset_y, classifier, x,y,index,learning_rate=0.01, batch_size=1, n_hidden=500):\n",
    "\n",
    "    train_set_x = theano.shared(np.asarray(dataset_X,\n",
    "                                            dtype=theano.config.floatX),\n",
    "                                borrow=True)\n",
    "    train_set_y = theano.shared(np.asarray(dataset_y,\n",
    "                                            dtype=theano.config.floatX),\n",
    "                                borrow=True)\n",
    "\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "\n",
    "\n",
    "    cost = (\n",
    "        classifier.mse(y)\n",
    "    )\n",
    "\n",
    "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "    \n",
    "    updates = [\n",
    "        (param, param - learning_rate * gparam)\n",
    "        for param, gparam in zip(classifier.params, gparams)\n",
    "    ]\n",
    "\n",
    "\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "  \n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    error = []\n",
    "    error_min = np.inf\n",
    "    for minibatch_index in range(n_train_batches):\n",
    "        minibatch_avg_cost = train_model(minibatch_index)\n",
    "        linear_1 = np.dot(dataset_X, classifier.hiddenLayer.W_h.eval()) +  classifier.hiddenLayer.b_h.eval()\n",
    "        h1 = relu(linear_1)\n",
    "        output_nn = np.dot(h1,classifier.output_layer.W.eval()) + classifier.output_layer.b.eval()\n",
    "        error.append(np.mean((output_nn - dataset_y)**2))\n",
    "        \n",
    "        '''if error[len(error)-1] < error_min:\n",
    "            error_min = error[len(error)-1]\n",
    "            best_W1 = classifier.hiddenLayer.W_h.eval()\n",
    "            best_b1 = classifier.hiddenLayer.b_h.eval()\n",
    "            best_Wo = classifier.output_layer.W.eval()\n",
    "            best_bo = classifier.output_layer.b.eval()'''\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    \n",
    "    return(error, classifier.hiddenLayer.W_h.eval(), classifier.hiddenLayer.b_h.eval(), classifier.output_layer.W.eval(), classifier.output_layer.b.eval())\n",
    "\n",
    "def predict_NN(X_input, W1, b1, Wo, bo):\n",
    "    return(np.dot(relu(np.dot(X_input,W1) + b1),Wo) + bo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        \n",
    "        \"\"\"Define max length of memory and gamma\"\"\"\n",
    "         \n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remind(self, states, game_over):\n",
    "\n",
    "        \"\"\"Add experience to memory\"\"\"\n",
    "        \n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, W1, b1, Wo, bo, batch_size=10):\n",
    "        \n",
    "        \"\"\"Get the batch input and targets we will train on\"\"\"\n",
    "        \n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = 2\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        \n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        \n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,\n",
    "                                                  size=inputs.shape[0])):\n",
    "            \n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "            inputs[i:i+1] = state_t\n",
    "            targets[i] = predict_NN(state_t, W1, b1, Wo, bo)\n",
    "            Q_sa = np.max(targets[i])\n",
    "            if game_over:\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CartPole on OpenAI Gym and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters : epsilon : 0.1 C : 50 , learning rate : 0.1 batch size for training : 50\n",
      "Training for  3000 time-steps ...\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "learning_rate=0.1\n",
    "epsilon = .1\n",
    "num_actions = env.action_space.n \n",
    "max_memory = 4000000000\n",
    "hidden_size = 200\n",
    "batch_size = 50\n",
    "acc_reward=0\n",
    "observation_shape = env.observation_space.shape[0]\n",
    "time_step=0\n",
    "max_time_steps= 3000\n",
    "everyC=50\n",
    "agent = Agent(max_memory=max_memory)\n",
    "win_cnt = 0\n",
    "t0 = time.time()\n",
    "actual_total=0\n",
    "e=0\n",
    "\n",
    "print('Parameters :','epsilon :', epsilon,'C :', everyC,', learning rate :', learning_rate, 'batch size for training :', batch_size)\n",
    "\n",
    "print('Training for ',max_time_steps,'time-steps ...')\n",
    "N = 6\n",
    "X = np.random.uniform(low=-5.,high=5.,size=(N, 4)).astype('float32')\n",
    "W = np.random.uniform(low=-5.,high=5.,size=(4, 2)).astype('float32')\n",
    "b = np.random.uniform(low=-5.,high=5.,size=2).astype('float32') \n",
    "\n",
    "noise = np.random.normal(0,1,(N,2))\n",
    "\n",
    "y = np.dot(X**2,W) + 5*np.dot(X,W) +  b + noise\n",
    "y=y.astype('float32')\n",
    "\n",
    "\n",
    "# allocate symbolic variables for the data\n",
    "index = T.lscalar()  \n",
    "x = T.matrix('x') \n",
    "y_g = T.matrix('y')  \n",
    "# construct the neural net\n",
    "classifier = Neural_Network(input=x,n_in=4,n_hidden=hidden_size,n_out=2)\n",
    "\n",
    "#initalize with a random backpropagation\n",
    "err, W1, b1, Wo, bo = train_on_batch(X, y, classifier, x=x,y=y_g,index=index, learning_rate=0.0003, batch_size=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 time steps done,  1 episodes done. Reward : 10.0 , loss : 0.2838515273475888\n",
      "20 time steps done,  2 episodes done. Reward : 10.0 , loss : 0.3254621928440241\n",
      "29 time steps done,  3 episodes done. Reward : 9.0 , loss : 0.2861200063510862\n",
      "40 time steps done,  4 episodes done. Reward : 11.0 , loss : 1.0596272859053957\n",
      "49 time steps done,  5 episodes done. Reward : 9.0 , loss : 0.3972365031910354\n",
      "59 time steps done,  6 episodes done. Reward : 10.0 , loss : 0.472534223405663\n",
      "68 time steps done,  7 episodes done. Reward : 9.0 , loss : 0.34968655711976415\n",
      "78 time steps done,  8 episodes done. Reward : 10.0 , loss : 0.45572449390716463\n",
      "88 time steps done,  9 episodes done. Reward : 10.0 , loss : 0.28305991453129276\n",
      "99 time steps done,  10 episodes done. Reward : 11.0 , loss : 0.7055340533128835\n",
      "108 time steps done,  11 episodes done. Reward : 9.0 , loss : 0.7484363789657678\n",
      "117 time steps done,  12 episodes done. Reward : 9.0 , loss : 0.4976061281048468\n",
      "126 time steps done,  13 episodes done. Reward : 9.0 , loss : 0.32466334171670014\n",
      "136 time steps done,  14 episodes done. Reward : 10.0 , loss : 0.3289494145926443\n",
      "146 time steps done,  15 episodes done. Reward : 10.0 , loss : 1.0663687129612245\n",
      "156 time steps done,  16 episodes done. Reward : 10.0 , loss : 0.5009053986883586\n",
      "168 time steps done,  17 episodes done. Reward : 12.0 , loss : 0.1476513269507812\n",
      "177 time steps done,  18 episodes done. Reward : 9.0 , loss : 0.7696869943736796\n",
      "187 time steps done,  19 episodes done. Reward : 10.0 , loss : 1.950585063133691\n",
      "196 time steps done,  20 episodes done. Reward : 9.0 , loss : 0.488841837498012\n",
      "205 time steps done,  21 episodes done. Reward : 9.0 , loss : 0.9842517082461275\n",
      "216 time steps done,  22 episodes done. Reward : 11.0 , loss : 0.4261828463080733\n",
      "226 time steps done,  23 episodes done. Reward : 10.0 , loss : 0.46974029599955885\n",
      "236 time steps done,  24 episodes done. Reward : 10.0 , loss : 0.3042370863492129\n",
      "247 time steps done,  25 episodes done. Reward : 11.0 , loss : 0.3032668337895315\n",
      "257 time steps done,  26 episodes done. Reward : 10.0 , loss : 0.4020447816375875\n",
      "267 time steps done,  27 episodes done. Reward : 10.0 , loss : 0.3493121317832298\n",
      "276 time steps done,  28 episodes done. Reward : 9.0 , loss : 0.34731240400521635\n",
      "285 time steps done,  29 episodes done. Reward : 9.0 , loss : 0.17190319797278786\n",
      "294 time steps done,  30 episodes done. Reward : 9.0 , loss : 0.24748663225188125\n",
      "303 time steps done,  31 episodes done. Reward : 9.0 , loss : 0.2546296261204517\n",
      "314 time steps done,  32 episodes done. Reward : 11.0 , loss : 0.2919170261564399\n",
      "323 time steps done,  33 episodes done. Reward : 9.0 , loss : 0.31653034802644575\n",
      "332 time steps done,  34 episodes done. Reward : 9.0 , loss : 0.6741191751733245\n",
      "342 time steps done,  35 episodes done. Reward : 10.0 , loss : 0.17233102455200708\n",
      "352 time steps done,  36 episodes done. Reward : 10.0 , loss : 0.2606188478099756\n",
      "362 time steps done,  37 episodes done. Reward : 10.0 , loss : 0.28990314888374785\n",
      "372 time steps done,  38 episodes done. Reward : 10.0 , loss : 0.3494441035518731\n",
      "381 time steps done,  39 episodes done. Reward : 9.0 , loss : 0.3383382931751165\n",
      "389 time steps done,  40 episodes done. Reward : 8.0 , loss : 0.4005876427484328\n",
      "402 time steps done,  41 episodes done. Reward : 13.0 , loss : 0.5056455221776803\n",
      "410 time steps done,  42 episodes done. Reward : 8.0 , loss : 0.25065872434799585\n",
      "420 time steps done,  43 episodes done. Reward : 10.0 , loss : 0.4925990713983998\n",
      "431 time steps done,  44 episodes done. Reward : 11.0 , loss : 0.4578349232844946\n",
      "441 time steps done,  45 episodes done. Reward : 10.0 , loss : 0.461466207421863\n",
      "450 time steps done,  46 episodes done. Reward : 9.0 , loss : 0.280162077436377\n",
      "458 time steps done,  47 episodes done. Reward : 8.0 , loss : 0.32380178316978664\n",
      "467 time steps done,  48 episodes done. Reward : 9.0 , loss : 0.18860560959429748\n",
      "477 time steps done,  49 episodes done. Reward : 10.0 , loss : 0.23026019475089934\n",
      "485 time steps done,  50 episodes done. Reward : 8.0 , loss : 0.2316009125398792\n",
      "495 time steps done,  51 episodes done. Reward : 10.0 , loss : 0.4441899900843041\n",
      "506 time steps done,  52 episodes done. Reward : 11.0 , loss : 0.24247444461224282\n",
      "515 time steps done,  53 episodes done. Reward : 9.0 , loss : 0.191907308311741\n",
      "524 time steps done,  54 episodes done. Reward : 9.0 , loss : 0.7916286645278676\n",
      "534 time steps done,  55 episodes done. Reward : 10.0 , loss : 0.5932225262596805\n",
      "544 time steps done,  56 episodes done. Reward : 10.0 , loss : 1.0269436866160233\n",
      "554 time steps done,  57 episodes done. Reward : 10.0 , loss : 0.5998631694790202\n",
      "566 time steps done,  58 episodes done. Reward : 12.0 , loss : 0.5753126362962764\n",
      "578 time steps done,  59 episodes done. Reward : 12.0 , loss : 0.2080232871061534\n",
      "587 time steps done,  60 episodes done. Reward : 9.0 , loss : 0.14760784943319982\n",
      "597 time steps done,  61 episodes done. Reward : 10.0 , loss : 0.16740818348639283\n",
      "606 time steps done,  62 episodes done. Reward : 9.0 , loss : 0.12371580394503569\n",
      "616 time steps done,  63 episodes done. Reward : 10.0 , loss : 0.30688110053903006\n",
      "625 time steps done,  64 episodes done. Reward : 9.0 , loss : 4.257951937936394\n",
      "635 time steps done,  65 episodes done. Reward : 10.0 , loss : 1.7594031895750677\n",
      "644 time steps done,  66 episodes done. Reward : 9.0 , loss : 0.94130062544695\n",
      "654 time steps done,  67 episodes done. Reward : 10.0 , loss : 0.31314057603251716\n",
      "666 time steps done,  68 episodes done. Reward : 12.0 , loss : 0.24893866580500873\n",
      "677 time steps done,  69 episodes done. Reward : 11.0 , loss : 0.2834462130598157\n",
      "688 time steps done,  70 episodes done. Reward : 11.0 , loss : 1.514054788503798\n",
      "696 time steps done,  71 episodes done. Reward : 8.0 , loss : 0.48340798398769136\n",
      "707 time steps done,  72 episodes done. Reward : 11.0 , loss : 2.6570180296152377\n",
      "716 time steps done,  73 episodes done. Reward : 9.0 , loss : 0.3698926030846462\n",
      "725 time steps done,  74 episodes done. Reward : 9.0 , loss : 0.923770266626838\n",
      "734 time steps done,  75 episodes done. Reward : 9.0 , loss : 1.8002368406329896\n",
      "746 time steps done,  76 episodes done. Reward : 12.0 , loss : 0.33751938142692106\n",
      "755 time steps done,  77 episodes done. Reward : 9.0 , loss : 0.7139090132987022\n",
      "766 time steps done,  78 episodes done. Reward : 11.0 , loss : 1.4641820609053264\n",
      "776 time steps done,  79 episodes done. Reward : 10.0 , loss : 0.4255956614212299\n",
      "785 time steps done,  80 episodes done. Reward : 9.0 , loss : 0.1732783120660444\n",
      "795 time steps done,  81 episodes done. Reward : 10.0 , loss : 4.594175591738066\n",
      "805 time steps done,  82 episodes done. Reward : 10.0 , loss : 0.9692764594459871\n",
      "819 time steps done,  83 episodes done. Reward : 14.0 , loss : 0.7029006136395455\n",
      "829 time steps done,  84 episodes done. Reward : 10.0 , loss : 0.22849737695431174\n",
      "839 time steps done,  85 episodes done. Reward : 10.0 , loss : 0.703311427157249\n",
      "849 time steps done,  86 episodes done. Reward : 10.0 , loss : 2.1227683849778343\n",
      "860 time steps done,  87 episodes done. Reward : 11.0 , loss : 0.2921618918036082\n",
      "870 time steps done,  88 episodes done. Reward : 10.0 , loss : 0.5300629983237228\n",
      "879 time steps done,  89 episodes done. Reward : 9.0 , loss : 1.0623383706650986\n",
      "890 time steps done,  90 episodes done. Reward : 11.0 , loss : 0.6994140076443837\n",
      "899 time steps done,  91 episodes done. Reward : 9.0 , loss : 0.574166522407063\n",
      "910 time steps done,  92 episodes done. Reward : 11.0 , loss : 0.2376281965699733\n",
      "919 time steps done,  93 episodes done. Reward : 9.0 , loss : 0.20941121919198136\n",
      "931 time steps done,  94 episodes done. Reward : 12.0 , loss : 3.6616387432705513\n",
      "940 time steps done,  95 episodes done. Reward : 9.0 , loss : 0.2864565100488275\n",
      "950 time steps done,  96 episodes done. Reward : 10.0 , loss : 2.269460755909039\n",
      "964 time steps done,  97 episodes done. Reward : 14.0 , loss : 3.3919121102289744\n",
      "977 time steps done,  98 episodes done. Reward : 13.0 , loss : 0.28651307098390716\n",
      "986 time steps done,  99 episodes done. Reward : 9.0 , loss : 0.25847586049474247\n",
      "1000 time steps done,  100 episodes done. Reward : 14.0 , loss : 0.3499076852373364\n",
      "1008 time steps done,  101 episodes done. Reward : 8.0 , loss : 0.2567670642359279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020 time steps done,  102 episodes done. Reward : 12.0 , loss : 1.7773737964387482\n",
      "1029 time steps done,  103 episodes done. Reward : 9.0 , loss : 0.276968870431352\n",
      "1039 time steps done,  104 episodes done. Reward : 10.0 , loss : 6.596524227082649\n",
      "1048 time steps done,  105 episodes done. Reward : 9.0 , loss : 0.26214960807023824\n",
      "1057 time steps done,  106 episodes done. Reward : 9.0 , loss : 0.6017012617575564\n",
      "1069 time steps done,  107 episodes done. Reward : 12.0 , loss : 0.3961594898730152\n",
      "1079 time steps done,  108 episodes done. Reward : 10.0 , loss : 7.312518334773865\n",
      "1093 time steps done,  109 episodes done. Reward : 14.0 , loss : 0.43872839464742597\n",
      "1106 time steps done,  110 episodes done. Reward : 13.0 , loss : 0.597903262196153\n",
      "1115 time steps done,  111 episodes done. Reward : 9.0 , loss : 2.281564420108711\n",
      "1132 time steps done,  112 episodes done. Reward : 17.0 , loss : 0.19721482243574734\n",
      "1143 time steps done,  113 episodes done. Reward : 11.0 , loss : 0.14763368135691793\n",
      "1157 time steps done,  114 episodes done. Reward : 14.0 , loss : 3.4231929340677425\n",
      "1173 time steps done,  115 episodes done. Reward : 16.0 , loss : 0.3381483367121701\n",
      "1185 time steps done,  116 episodes done. Reward : 12.0 , loss : 0.48895917128639804\n",
      "1193 time steps done,  117 episodes done. Reward : 8.0 , loss : 0.15514717106287032\n",
      "1202 time steps done,  118 episodes done. Reward : 9.0 , loss : 0.5413630465851945\n",
      "1214 time steps done,  119 episodes done. Reward : 12.0 , loss : 0.25692159341963405\n",
      "1226 time steps done,  120 episodes done. Reward : 12.0 , loss : 0.6323331137075556\n",
      "1234 time steps done,  121 episodes done. Reward : 8.0 , loss : 0.5801312061059108\n",
      "1247 time steps done,  122 episodes done. Reward : 13.0 , loss : 0.2911505629774239\n",
      "1262 time steps done,  123 episodes done. Reward : 15.0 , loss : 1.0979248127181562\n",
      "1272 time steps done,  124 episodes done. Reward : 10.0 , loss : 0.24181331680050333\n",
      "1286 time steps done,  125 episodes done. Reward : 14.0 , loss : 0.1646730245236755\n",
      "1300 time steps done,  126 episodes done. Reward : 14.0 , loss : 0.28704216770015845\n",
      "1313 time steps done,  127 episodes done. Reward : 13.0 , loss : 1.0514320983803978\n",
      "1332 time steps done,  128 episodes done. Reward : 19.0 , loss : 0.2271326893479428\n",
      "1343 time steps done,  129 episodes done. Reward : 11.0 , loss : 0.21988750324735787\n",
      "1355 time steps done,  130 episodes done. Reward : 12.0 , loss : 0.35749159089577787\n",
      "1366 time steps done,  131 episodes done. Reward : 11.0 , loss : 0.09559328635371499\n",
      "1376 time steps done,  132 episodes done. Reward : 10.0 , loss : 0.20838189073142252\n",
      "1390 time steps done,  133 episodes done. Reward : 14.0 , loss : 0.22004232436409799\n",
      "1399 time steps done,  134 episodes done. Reward : 9.0 , loss : 1.4559491284865458\n",
      "1408 time steps done,  135 episodes done. Reward : 9.0 , loss : 0.4843836396838727\n",
      "1417 time steps done,  136 episodes done. Reward : 9.0 , loss : 0.9340955107170636\n",
      "1427 time steps done,  137 episodes done. Reward : 10.0 , loss : 0.16256144098239667\n",
      "1436 time steps done,  138 episodes done. Reward : 9.0 , loss : 0.40919399473352386\n",
      "1448 time steps done,  139 episodes done. Reward : 12.0 , loss : 0.38023665976957277\n",
      "1463 time steps done,  140 episodes done. Reward : 15.0 , loss : 0.34996099651601403\n",
      "1485 time steps done,  141 episodes done. Reward : 22.0 , loss : 1.6546965598312866\n",
      "1493 time steps done,  142 episodes done. Reward : 8.0 , loss : 0.6343645982241806\n",
      "1503 time steps done,  143 episodes done. Reward : 10.0 , loss : 0.8570710728776415\n",
      "1512 time steps done,  144 episodes done. Reward : 9.0 , loss : 0.4495540945515428\n",
      "1524 time steps done,  145 episodes done. Reward : 12.0 , loss : 0.4425539454829417\n",
      "1535 time steps done,  146 episodes done. Reward : 11.0 , loss : 2.052551583581474\n",
      "1547 time steps done,  147 episodes done. Reward : 12.0 , loss : 0.12710491194568113\n",
      "1557 time steps done,  148 episodes done. Reward : 10.0 , loss : 0.2205875394227184\n",
      "1570 time steps done,  149 episodes done. Reward : 13.0 , loss : 0.2135233555455767\n",
      "1580 time steps done,  150 episodes done. Reward : 10.0 , loss : 2.0461652331688907\n",
      "1590 time steps done,  151 episodes done. Reward : 10.0 , loss : 1.122761067824114\n",
      "1601 time steps done,  152 episodes done. Reward : 11.0 , loss : 1.565962093397188\n",
      "1618 time steps done,  153 episodes done. Reward : 17.0 , loss : 0.3473606884833433\n",
      "1626 time steps done,  154 episodes done. Reward : 8.0 , loss : 0.1484664038756308\n",
      "1636 time steps done,  155 episodes done. Reward : 10.0 , loss : 0.7722961001600879\n",
      "1646 time steps done,  156 episodes done. Reward : 10.0 , loss : 0.21592160207372657\n",
      "1662 time steps done,  157 episodes done. Reward : 16.0 , loss : 0.14022502914000826\n",
      "1672 time steps done,  158 episodes done. Reward : 10.0 , loss : 0.9638783557783087\n",
      "1682 time steps done,  159 episodes done. Reward : 10.0 , loss : 0.5154930258716215\n",
      "1694 time steps done,  160 episodes done. Reward : 12.0 , loss : 1.2447807748707627\n",
      "1704 time steps done,  161 episodes done. Reward : 10.0 , loss : 0.20457555224214163\n",
      "1714 time steps done,  162 episodes done. Reward : 10.0 , loss : 0.7223047116946795\n",
      "1724 time steps done,  163 episodes done. Reward : 10.0 , loss : 0.19717326389086587\n",
      "1739 time steps done,  164 episodes done. Reward : 15.0 , loss : 1.147178655873076\n",
      "1749 time steps done,  165 episodes done. Reward : 10.0 , loss : 2.6574845560037854\n",
      "1759 time steps done,  166 episodes done. Reward : 10.0 , loss : 0.4761990036551728\n",
      "1770 time steps done,  167 episodes done. Reward : 11.0 , loss : 0.14514472131271328\n",
      "1783 time steps done,  168 episodes done. Reward : 13.0 , loss : 0.36053124081872295\n",
      "1794 time steps done,  169 episodes done. Reward : 11.0 , loss : 0.4687298447944116\n",
      "1805 time steps done,  170 episodes done. Reward : 11.0 , loss : 1.2898842708834188\n",
      "1816 time steps done,  171 episodes done. Reward : 11.0 , loss : 2.157584482349734\n",
      "1827 time steps done,  172 episodes done. Reward : 11.0 , loss : 0.2118805946987779\n",
      "1836 time steps done,  173 episodes done. Reward : 9.0 , loss : 0.8083910129987373\n",
      "1851 time steps done,  174 episodes done. Reward : 15.0 , loss : 0.05362814144356804\n",
      "1862 time steps done,  175 episodes done. Reward : 11.0 , loss : 0.41725945070367554\n",
      "1875 time steps done,  176 episodes done. Reward : 13.0 , loss : 1.846595290187059\n",
      "1886 time steps done,  177 episodes done. Reward : 11.0 , loss : 0.23282319258231976\n",
      "1895 time steps done,  178 episodes done. Reward : 9.0 , loss : 0.44957492098093693\n",
      "1905 time steps done,  179 episodes done. Reward : 10.0 , loss : 0.6474872991960515\n",
      "1913 time steps done,  180 episodes done. Reward : 8.0 , loss : 0.4466164790800596\n",
      "1921 time steps done,  181 episodes done. Reward : 8.0 , loss : 2.02718427030178\n",
      "1930 time steps done,  182 episodes done. Reward : 9.0 , loss : 0.3827903303263571\n",
      "1940 time steps done,  183 episodes done. Reward : 10.0 , loss : 0.8347180415174129\n",
      "1951 time steps done,  184 episodes done. Reward : 11.0 , loss : 3.082690777795504\n",
      "1964 time steps done,  185 episodes done. Reward : 13.0 , loss : 0.8076765271173201\n",
      "1974 time steps done,  186 episodes done. Reward : 10.0 , loss : 0.31310044764979117\n",
      "1982 time steps done,  187 episodes done. Reward : 8.0 , loss : 0.43010079143366936\n",
      "1993 time steps done,  188 episodes done. Reward : 11.0 , loss : 0.31569309756962466\n",
      "2005 time steps done,  189 episodes done. Reward : 12.0 , loss : 0.1693538644798901\n",
      "2018 time steps done,  190 episodes done. Reward : 13.0 , loss : 0.520299051413842\n",
      "2029 time steps done,  191 episodes done. Reward : 11.0 , loss : 0.8345448366647636\n",
      "2038 time steps done,  192 episodes done. Reward : 9.0 , loss : 0.23151470695567236\n",
      "2048 time steps done,  193 episodes done. Reward : 10.0 , loss : 0.6325639060403051\n",
      "2060 time steps done,  194 episodes done. Reward : 12.0 , loss : 0.6321370285893534\n",
      "2069 time steps done,  195 episodes done. Reward : 9.0 , loss : 0.31463523842727176\n",
      "2079 time steps done,  196 episodes done. Reward : 10.0 , loss : 0.28545846009324594\n",
      "2090 time steps done,  197 episodes done. Reward : 11.0 , loss : 1.6307132031597629\n",
      "2102 time steps done,  198 episodes done. Reward : 12.0 , loss : 0.3509039960044232\n",
      "2115 time steps done,  199 episodes done. Reward : 13.0 , loss : 0.3103622628846686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2124 time steps done,  200 episodes done. Reward : 9.0 , loss : 0.09634877412424968\n",
      "2133 time steps done,  201 episodes done. Reward : 9.0 , loss : 0.6569694689002872\n",
      "2142 time steps done,  202 episodes done. Reward : 9.0 , loss : 0.3616266936831243\n",
      "2167 time steps done,  203 episodes done. Reward : 25.0 , loss : 0.8119658401396588\n",
      "2177 time steps done,  204 episodes done. Reward : 10.0 , loss : 0.2240716148254977\n",
      "2189 time steps done,  205 episodes done. Reward : 12.0 , loss : 0.8874326862254481\n",
      "2204 time steps done,  206 episodes done. Reward : 15.0 , loss : 0.4660330465355031\n",
      "2227 time steps done,  207 episodes done. Reward : 23.0 , loss : 0.669444006189769\n",
      "2240 time steps done,  208 episodes done. Reward : 13.0 , loss : 0.5967556229240834\n",
      "2251 time steps done,  209 episodes done. Reward : 11.0 , loss : 0.997277704330449\n",
      "2262 time steps done,  210 episodes done. Reward : 11.0 , loss : 1.7172424270024704\n",
      "2270 time steps done,  211 episodes done. Reward : 8.0 , loss : 0.15114188194027112\n",
      "2281 time steps done,  212 episodes done. Reward : 11.0 , loss : 0.8959643651461154\n",
      "2292 time steps done,  213 episodes done. Reward : 11.0 , loss : 1.2900862839114908\n",
      "2306 time steps done,  214 episodes done. Reward : 14.0 , loss : 0.274506695553513\n",
      "2316 time steps done,  215 episodes done. Reward : 10.0 , loss : 0.9261564260713819\n",
      "2328 time steps done,  216 episodes done. Reward : 12.0 , loss : 1.4265067616773197\n",
      "2337 time steps done,  217 episodes done. Reward : 9.0 , loss : 0.12777475656526247\n",
      "2351 time steps done,  218 episodes done. Reward : 14.0 , loss : 0.35436381444459353\n",
      "2363 time steps done,  219 episodes done. Reward : 12.0 , loss : 0.36911595964012833\n",
      "2375 time steps done,  220 episodes done. Reward : 12.0 , loss : 0.23585738127327371\n",
      "2386 time steps done,  221 episodes done. Reward : 11.0 , loss : 0.8310172279662206\n",
      "2402 time steps done,  222 episodes done. Reward : 16.0 , loss : 1.2920540935360123\n",
      "2419 time steps done,  223 episodes done. Reward : 17.0 , loss : 1.686533256465549\n",
      "2428 time steps done,  224 episodes done. Reward : 9.0 , loss : 0.9820453120772099\n",
      "2449 time steps done,  225 episodes done. Reward : 21.0 , loss : 0.7692010100367851\n",
      "2464 time steps done,  226 episodes done. Reward : 15.0 , loss : 0.5790019035850225\n",
      "2476 time steps done,  227 episodes done. Reward : 12.0 , loss : 0.34700734788051707\n",
      "2487 time steps done,  228 episodes done. Reward : 11.0 , loss : 0.14158431741207814\n",
      "2510 time steps done,  229 episodes done. Reward : 23.0 , loss : 0.24898284355768663\n",
      "2519 time steps done,  230 episodes done. Reward : 9.0 , loss : 0.6459268237801209\n",
      "2534 time steps done,  231 episodes done. Reward : 15.0 , loss : 0.22447459224121616\n",
      "2547 time steps done,  232 episodes done. Reward : 13.0 , loss : 0.4233741781641314\n",
      "2559 time steps done,  233 episodes done. Reward : 12.0 , loss : 0.19791926426927745\n",
      "2567 time steps done,  234 episodes done. Reward : 8.0 , loss : 0.05519741865411195\n",
      "2578 time steps done,  235 episodes done. Reward : 11.0 , loss : 1.2923211313059273\n",
      "2588 time steps done,  236 episodes done. Reward : 10.0 , loss : 0.2726161509803577\n",
      "2599 time steps done,  237 episodes done. Reward : 11.0 , loss : 0.4238388780941946\n",
      "2610 time steps done,  238 episodes done. Reward : 11.0 , loss : 0.48686479421686685\n",
      "2626 time steps done,  239 episodes done. Reward : 16.0 , loss : 0.34021358982785443\n",
      "2637 time steps done,  240 episodes done. Reward : 11.0 , loss : 2.4463549157898106\n",
      "2664 time steps done,  241 episodes done. Reward : 27.0 , loss : 0.2175534402901549\n",
      "2673 time steps done,  242 episodes done. Reward : 9.0 , loss : 0.15183687376838503\n",
      "2683 time steps done,  243 episodes done. Reward : 10.0 , loss : 0.9804427193426946\n",
      "2694 time steps done,  244 episodes done. Reward : 11.0 , loss : 0.8763884112732466\n",
      "2704 time steps done,  245 episodes done. Reward : 10.0 , loss : 1.2210999836062204\n",
      "2721 time steps done,  246 episodes done. Reward : 17.0 , loss : 0.37670882781555776\n",
      "2730 time steps done,  247 episodes done. Reward : 9.0 , loss : 0.4199868095429871\n",
      "2749 time steps done,  248 episodes done. Reward : 19.0 , loss : 1.8324040989786443\n",
      "2762 time steps done,  249 episodes done. Reward : 13.0 , loss : 0.26772388092488625\n",
      "2773 time steps done,  250 episodes done. Reward : 11.0 , loss : 0.14355838356751693\n",
      "2782 time steps done,  251 episodes done. Reward : 9.0 , loss : 0.19395044530436084\n",
      "2791 time steps done,  252 episodes done. Reward : 9.0 , loss : 0.20331510748692472\n",
      "2804 time steps done,  253 episodes done. Reward : 13.0 , loss : 1.2058485207796212\n",
      "2813 time steps done,  254 episodes done. Reward : 9.0 , loss : 0.19669592565719313\n",
      "2823 time steps done,  255 episodes done. Reward : 10.0 , loss : 1.0661727101972558\n",
      "2833 time steps done,  256 episodes done. Reward : 10.0 , loss : 0.8142559068432692\n",
      "2843 time steps done,  257 episodes done. Reward : 10.0 , loss : 0.14315075768650862\n",
      "2859 time steps done,  258 episodes done. Reward : 16.0 , loss : 0.6347147950617295\n",
      "2875 time steps done,  259 episodes done. Reward : 16.0 , loss : 0.07219017087147131\n",
      "2885 time steps done,  260 episodes done. Reward : 10.0 , loss : 2.099253090661695\n",
      "2893 time steps done,  261 episodes done. Reward : 8.0 , loss : 0.3515838234757384\n",
      "2904 time steps done,  262 episodes done. Reward : 11.0 , loss : 0.3801435085813696\n",
      "2915 time steps done,  263 episodes done. Reward : 11.0 , loss : 0.3638393354387107\n",
      "2925 time steps done,  264 episodes done. Reward : 10.0 , loss : 0.6915234416079303\n",
      "2934 time steps done,  265 episodes done. Reward : 9.0 , loss : 0.2750183920765734\n",
      "2944 time steps done,  266 episodes done. Reward : 10.0 , loss : 0.25158400622682836\n",
      "2957 time steps done,  267 episodes done. Reward : 13.0 , loss : 0.5090240757297632\n",
      "2968 time steps done,  268 episodes done. Reward : 11.0 , loss : 0.30301247471739273\n",
      "2985 time steps done,  269 episodes done. Reward : 17.0 , loss : 0.09754553357146727\n",
      "2994 time steps done,  270 episodes done. Reward : 9.0 , loss : 2.0472193332870354\n",
      "3007 time steps done,  271 episodes done. Reward : 13.0 , loss : 0.1656992330013839\n",
      "Total training time : 947.6403286457062 Actual training time : 929.0736656188965\n",
      "Win ratio (nb of games won/nb of games played) : 0.0\n"
     ]
    }
   ],
   "source": [
    "while time_step<max_time_steps:\n",
    "    loss = 0.\n",
    "    acc_reward = 0\n",
    "    C=0\n",
    "    e+=1\n",
    "    input_t = env.reset()\n",
    "    input_t = input_t.reshape((1,observation_shape))\n",
    "    game_over = False\n",
    "    \n",
    "    while not game_over:\n",
    "\n",
    "        input_tm1 = input_t.astype('float32')\n",
    "        \n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(0, num_actions, size=1)[0]\n",
    "        else:\n",
    "            q = predict_NN(input_tm1, W1, b1, Wo, bo)\n",
    "            action = np.argmax(q)\n",
    "\n",
    "        input_t, reward, game_over, infodemerde = env.step(action)\n",
    "        input_t = input_t.reshape((1,observation_shape))\n",
    "        \n",
    "        acc_reward += reward\n",
    "\n",
    "        agent.remind([input_tm1, action, reward, input_t], game_over)\n",
    "\n",
    "        inputs, targets = agent.get_batch(W1, b1, Wo, bo, batch_size=batch_size)\n",
    "        inputs=inputs.astype('float32')\n",
    "        targets=targets.astype('float32')\n",
    "\n",
    "        t2 = time.time()  #start of actual training time\n",
    "\n",
    "        #TRAIN\n",
    "        err, W1, b1, Wo, bo = train_on_batch(inputs, targets, classifier, x=x, y=y_g, index=index, learning_rate=learning_rate, batch_size=len(inputs))\n",
    "\n",
    "        t3 = time.time() #end of actual training time\n",
    "        actual_total += t3-t2\n",
    "\n",
    "        time_step += 1\n",
    "\n",
    "        if acc_reward>=200:\n",
    "            game_over=True\n",
    "            win_cnt+=1\n",
    "\n",
    "    print(time_step,'time steps done, ',e,'episodes done. Reward :', acc_reward, ', loss :', err[0])\n",
    "\n",
    "t1 = time.time() #end of training time\n",
    "total = t1-t0\n",
    "print('Total training time :', total,'Actual training time :', actual_total)\n",
    "print('Win ratio (nb of games won/nb of games played) :', win_cnt/e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for  10 episodes ...\n",
      "0 episodes done. Reward : 10.0\n",
      "1 episodes done. Reward : 10.0\n",
      "2 episodes done. Reward : 10.0\n",
      "3 episodes done. Reward : 9.0\n",
      "4 episodes done. Reward : 8.0\n",
      "5 episodes done. Reward : 9.0\n",
      "6 episodes done. Reward : 10.0\n",
      "7 episodes done. Reward : 9.0\n",
      "8 episodes done. Reward : 10.0\n",
      "9 episodes done. Reward : 10.0\n",
      "The average reward over the test was : 9.5\n"
     ]
    }
   ],
   "source": [
    "#nb of episodes to test\n",
    "nb_e_test=10\n",
    "\n",
    "#Total reward over the episodes\n",
    "total_rew=0\n",
    "\n",
    "print('Testing for ',nb_e_test,'episodes ...')\n",
    "\n",
    "for episode in range(nb_e_test):\n",
    "    acc_reward = 0\n",
    "    input_t = env.reset()\n",
    "    input_t = input_t.reshape((1,observation_shape))\n",
    "    game_over = False\n",
    "\n",
    "    while not game_over:\n",
    "        input_tm1 = input_t.astype('float32')\n",
    "        \n",
    "        q = predict_NN(input_tm1, W1, b1, Wo, bo)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        input_t, reward, game_over, infodemerde = env.step(action)\n",
    "        input_t = input_t.reshape((1,observation_shape))\n",
    "        \n",
    "        acc_reward += reward\n",
    "\n",
    "        if acc_reward>=200:\n",
    "            game_over=True\n",
    "            \n",
    "    total_rew+=acc_reward\n",
    "\n",
    "    print(episode,'episodes done. Reward :', acc_reward)\n",
    "\n",
    "\n",
    "print('The average reward over the test was :',total_rew/nb_e_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
