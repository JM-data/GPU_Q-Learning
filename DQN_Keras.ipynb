{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import theano\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import math\n",
    "import json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "from keras.models import model_from_json\n",
    "import time\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "     \n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        \"\"\"Define max length of memory and gamma\"\"\"\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remind(self, states, game_over):      \n",
    "        \"\"\"Add experience to memory\"\"\"\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:         #Delete the first experience if the memory is too long\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, target_model, model, batch_size=10):\n",
    "        \"\"\"Get the batch input and targets we will train on\"\"\"\n",
    "        len_memory = len(self.memory)           #length of memory vector\n",
    "        num_actions = model.output_shape[-1]    #number of actions in action space\n",
    "        \n",
    "                                                #states is an experience : [input_t_minus_1, action, reward, input_t],\n",
    "        env_dim = self.memory[0][0][0].shape[1] #so memory[0] is state and memory[0][0][0].shape[1] is the size of the input\n",
    "        \n",
    "                                                \n",
    "        inputs = np.zeros((min(len_memory, batch_size),#if batch_size<len_memory (it is mostly the case), \n",
    "                           env_dim))                   #then input is a matrix with batch_size rows and size of obs columns\n",
    "        \n",
    "                                                          #Targets is a matrix with batch_size rows and \n",
    "        targets = np.zeros((inputs.shape[0], num_actions))#number of actions columns\n",
    "        \n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,\n",
    "                                                  size=inputs.shape[0])):\n",
    "            \n",
    "            #get experience number idx, idx being a random number in [0,length of memory]\n",
    "            #There are batch_size experiences that are drawn\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            \n",
    "            game_over = self.memory[idx][1]     #Is the game over ? if done in gym\n",
    "\n",
    "            inputs[i:i+1] = state_t             #The inputs of the NN are the state of the experience drawn\n",
    "            \n",
    "                                                          # target_model.predict(state_t)[0] is the \n",
    "            targets[i] = target_model.predict(state_t)[0] #vector of Q(state_t) for each action \n",
    "            \n",
    "                                                #Q_sa=Q_target(s,argmax_a'{Q(s',a')}\n",
    "                                                #index is the action you that maximizes the Q-value of the current network\n",
    "            index, maxima = max(enumerate(model.predict(state_tp1)[0]), key=operator.itemgetter(1))\n",
    "                                                            #We take the value of the target\n",
    "            Q_sa = target_model.predict(state_tp1)[0][index]#network for action index\n",
    "            \n",
    "            if game_over:  # if game_over is True\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                                                                       # the target for this particular experience is : \n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa #reward_t + gamma * max_a' Q(s', a')\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Envirtonment and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')  \n",
    "\n",
    "learning_rate=0.001             #learning rate\n",
    "epsilon = .1                    #exploration parameter\n",
    "num_actions = env.action_space.n#Number of possible actions\n",
    "max_memory = 4000000000         #Length of memory\n",
    "hidden_size = 200               #Number of hidden units\n",
    "batch_size = 50                 #Size of batch for training\n",
    "acc_reward=0                    #Accumulated reward over epoch\n",
    "time_step=0                     #counter of time-steps\n",
    "max_time_steps=2000             #total number of time-steps to train on\n",
    "everyC=5                        #Number of times we update the target network\n",
    "C=0                             #Parameter C\n",
    "\n",
    "#shape of observations\n",
    "observation_shape = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adam=keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, clipvalue=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Current DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=observation_shape, activation='relu')) #first fully connected layer, activation RELU\n",
    "model.add(Dense(num_actions))                                         #last fully connected layer, output Q(s,a,theta)\n",
    "\n",
    "model.compile(optimizer=Adam, loss='mean_squared_error')              #choose optimization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Target DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = Sequential()\n",
    "target_model.add(Dense(200, input_dim=observation_shape, activation='relu'))#first fully connected layer, activation RELU\n",
    "target_model.add(Dense(num_actions))                                        #last fully connected layer, output Q(s,a,theta)\n",
    "target_model.compile(optimizer=Adam, loss='mean_squared_error')             #choose optimization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Initializing the Experience Replay object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters : epsilon : 0.1 C : 5 , learning rate : 0.001 batch size for training : 50\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 200)               1000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 1,402\n",
      "Trainable params: 1,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for  2000 time-steps ...\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(max_memory=max_memory)\n",
    "\n",
    "win_cnt = 0      #nb of games won\n",
    "t0 = time.time() #start of traning time\n",
    "actual_total=0   #actual training time\n",
    "e=0              #nb of episodes\n",
    "\n",
    "print('Parameters :','epsilon :', epsilon,'C :', everyC,', learning rate :', learning_rate, 'batch size for training :', batch_size)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print('Training for ',max_time_steps,'time-steps ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 time steps done,  1 episodes done. Reward : 10.0 , loss : 4.714401543140411\n",
      "19 time steps done,  2 episodes done. Reward : 9.0 , loss : 3.815333664417267\n",
      "28 time steps done,  3 episodes done. Reward : 9.0 , loss : 4.476900339126587\n",
      "39 time steps done,  4 episodes done. Reward : 11.0 , loss : 7.107419312000275\n",
      "49 time steps done,  5 episodes done. Reward : 10.0 , loss : 8.374745905399323\n",
      "59 time steps done,  6 episodes done. Reward : 10.0 , loss : 10.59276008605957\n",
      "68 time steps done,  7 episodes done. Reward : 9.0 , loss : 12.984484612941742\n",
      "76 time steps done,  8 episodes done. Reward : 8.0 , loss : 10.284712731838226\n",
      "86 time steps done,  9 episodes done. Reward : 10.0 , loss : 16.13631683588028\n",
      "97 time steps done,  10 episodes done. Reward : 11.0 , loss : 25.485287189483643\n",
      "110 time steps done,  11 episodes done. Reward : 13.0 , loss : 23.956787943840027\n",
      "119 time steps done,  12 episodes done. Reward : 9.0 , loss : 15.109153747558594\n",
      "131 time steps done,  13 episodes done. Reward : 12.0 , loss : 18.523943662643433\n",
      "142 time steps done,  14 episodes done. Reward : 11.0 , loss : 10.24025297164917\n",
      "154 time steps done,  15 episodes done. Reward : 12.0 , loss : 12.734391927719116\n",
      "163 time steps done,  16 episodes done. Reward : 9.0 , loss : 6.5927122831344604\n",
      "173 time steps done,  17 episodes done. Reward : 10.0 , loss : 8.986398458480835\n",
      "182 time steps done,  18 episodes done. Reward : 9.0 , loss : 6.230116903781891\n",
      "191 time steps done,  19 episodes done. Reward : 9.0 , loss : 5.869564443826675\n",
      "200 time steps done,  20 episodes done. Reward : 9.0 , loss : 4.449896037578583\n",
      "212 time steps done,  21 episodes done. Reward : 12.0 , loss : 5.019683688879013\n",
      "222 time steps done,  22 episodes done. Reward : 10.0 , loss : 4.429200172424316\n",
      "231 time steps done,  23 episodes done. Reward : 9.0 , loss : 4.243792921304703\n",
      "240 time steps done,  24 episodes done. Reward : 9.0 , loss : 2.8925217241048813\n",
      "250 time steps done,  25 episodes done. Reward : 10.0 , loss : 3.739801749587059\n",
      "259 time steps done,  26 episodes done. Reward : 9.0 , loss : 2.9236061722040176\n",
      "269 time steps done,  27 episodes done. Reward : 10.0 , loss : 3.6201492249965668\n",
      "279 time steps done,  28 episodes done. Reward : 10.0 , loss : 3.3996516168117523\n",
      "290 time steps done,  29 episodes done. Reward : 11.0 , loss : 2.7828657552599907\n",
      "299 time steps done,  30 episodes done. Reward : 9.0 , loss : 2.4952264577150345\n",
      "308 time steps done,  31 episodes done. Reward : 9.0 , loss : 2.3020356744527817\n",
      "318 time steps done,  32 episodes done. Reward : 10.0 , loss : 2.2568619400262833\n",
      "328 time steps done,  33 episodes done. Reward : 10.0 , loss : 2.0003266856074333\n",
      "340 time steps done,  34 episodes done. Reward : 12.0 , loss : 2.008545145392418\n",
      "349 time steps done,  35 episodes done. Reward : 9.0 , loss : 1.7734959423542023\n",
      "360 time steps done,  36 episodes done. Reward : 11.0 , loss : 1.788393497467041\n",
      "369 time steps done,  37 episodes done. Reward : 9.0 , loss : 1.9469988942146301\n",
      "378 time steps done,  38 episodes done. Reward : 9.0 , loss : 1.5838008671998978\n",
      "391 time steps done,  39 episodes done. Reward : 13.0 , loss : 2.129636727273464\n",
      "403 time steps done,  40 episodes done. Reward : 12.0 , loss : 1.210189450532198\n",
      "415 time steps done,  41 episodes done. Reward : 12.0 , loss : 1.5477341040968895\n",
      "424 time steps done,  42 episodes done. Reward : 9.0 , loss : 1.0247460827231407\n",
      "435 time steps done,  43 episodes done. Reward : 11.0 , loss : 1.283730585128069\n",
      "445 time steps done,  44 episodes done. Reward : 10.0 , loss : 1.3737712129950523\n",
      "457 time steps done,  45 episodes done. Reward : 12.0 , loss : 1.5889487639069557\n",
      "468 time steps done,  46 episodes done. Reward : 11.0 , loss : 1.5448013618588448\n",
      "478 time steps done,  47 episodes done. Reward : 10.0 , loss : 0.9575757011771202\n",
      "488 time steps done,  48 episodes done. Reward : 10.0 , loss : 1.044457122683525\n",
      "498 time steps done,  49 episodes done. Reward : 10.0 , loss : 1.2499076277017593\n",
      "509 time steps done,  50 episodes done. Reward : 11.0 , loss : 1.1478095948696136\n",
      "520 time steps done,  51 episodes done. Reward : 11.0 , loss : 1.140810202807188\n",
      "534 time steps done,  52 episodes done. Reward : 14.0 , loss : 1.7299438416957855\n",
      "546 time steps done,  53 episodes done. Reward : 12.0 , loss : 1.124547552317381\n",
      "557 time steps done,  54 episodes done. Reward : 11.0 , loss : 1.3308076411485672\n",
      "568 time steps done,  55 episodes done. Reward : 11.0 , loss : 1.3237809352576733\n",
      "588 time steps done,  56 episodes done. Reward : 20.0 , loss : 2.160180240869522\n",
      "620 time steps done,  57 episodes done. Reward : 32.0 , loss : 3.2892859019339085\n",
      "652 time steps done,  58 episodes done. Reward : 32.0 , loss : 2.994699865579605\n",
      "686 time steps done,  59 episodes done. Reward : 34.0 , loss : 3.561157088726759\n",
      "732 time steps done,  60 episodes done. Reward : 46.0 , loss : 4.839599808678031\n",
      "789 time steps done,  61 episodes done. Reward : 57.0 , loss : 6.459437793120742\n",
      "812 time steps done,  62 episodes done. Reward : 23.0 , loss : 3.3903567381203175\n",
      "832 time steps done,  63 episodes done. Reward : 20.0 , loss : 6.618145637214184\n",
      "959 time steps done,  64 episodes done. Reward : 127.0 , loss : 44.082654455676675\n",
      "1023 time steps done,  65 episodes done. Reward : 64.0 , loss : 10.903249852359295\n",
      "1067 time steps done,  66 episodes done. Reward : 44.0 , loss : 18.481812302023172\n",
      "1108 time steps done,  67 episodes done. Reward : 41.0 , loss : 10.97649664990604\n",
      "1160 time steps done,  68 episodes done. Reward : 52.0 , loss : 10.741074156016111\n",
      "1236 time steps done,  69 episodes done. Reward : 76.0 , loss : 19.745340818539262\n",
      "1401 time steps done,  70 episodes done. Reward : 165.0 , loss : 37.2288019945845\n",
      "1601 time steps done,  71 episodes done. Reward : 200.0 , loss : 30.415006747469306\n",
      "1777 time steps done,  72 episodes done. Reward : 176.0 , loss : 40.636619959492236\n",
      "1976 time steps done,  73 episodes done. Reward : 199.0 , loss : 39.78361485246569\n",
      "2176 time steps done,  74 episodes done. Reward : 200.0 , loss : 38.855179293081164\n",
      "Total training time : 299.6915428638458 Actual training time : 7.712774991989136\n",
      "Win ratio (nb of games won/nb of games played) : 0.02702702702702703\n"
     ]
    }
   ],
   "source": [
    "while time_step<max_time_steps:\n",
    "    loss = 0.       #Set loss to zero\n",
    "    acc_reward = 0  #Set accumulated reward to 0\n",
    "    C=0             #Set C to zero\n",
    "    e+=1            #Add episode\n",
    "    \n",
    "    input_t = env.reset()\n",
    "    input_t = input_t.reshape((1,observation_shape))\n",
    "    \n",
    "    game_over = False #Since it's the beginning of the game, game_over is not True\n",
    "\n",
    "    while not game_over:\n",
    "        \n",
    "        input_tm1 = input_t            #set this state to be the last state\n",
    "        if np.random.rand() <= epsilon:# get next action according to espilon-greedy policy\n",
    "            #exploration\n",
    "            action = np.random.randint(0, num_actions, size=1)[0]\n",
    "        else:\n",
    "            #exploitation\n",
    "            q = model.predict(input_tm1)\n",
    "            action = np.argmax(q[0])\n",
    "\n",
    "        input_t, reward, game_over, infodemerde = env.step(action) #apply action, get rewards and new state\n",
    "        input_t = input_t.reshape((1,observation_shape))\n",
    "        \n",
    "        acc_reward += reward    #Accumulate reward\n",
    "\n",
    "        agent.remind([input_tm1, action, reward, input_t], game_over) # store experience\n",
    "        \n",
    "        #Create new target network every C updates, by cloning the current network\n",
    "        if C%everyC==0:\n",
    "            model.save_weights(\"model_cartpole_TARGET\", overwrite=True)\n",
    "            with open(\"model_cartpole_TARGET.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile) \n",
    "            target_model.load_weights(\"model_cartpole_TARGET\")\n",
    "            \n",
    "\n",
    "        C += 1         #Increment C\n",
    "        # get batch we will train on\n",
    "        inputs, targets = agent.get_batch(target_model, model, batch_size=batch_size)\n",
    "\n",
    "        t2 = time.time() #start of actual training time\n",
    "        loss += model.train_on_batch(inputs, targets)\n",
    "        t3 = time.time() #end of actual training time\n",
    "        actual_total += t3-t2\n",
    "        time_step += 1   #increment time-step\n",
    "\n",
    "        if acc_reward>=200: #end game if max score is reached\n",
    "            game_over=True\n",
    "            win_cnt+=1\n",
    "\n",
    "    print(time_step,'time steps done, ',e,'episodes done. Reward :', acc_reward, ', loss :', loss)\n",
    "\n",
    "t1 = time.time() #end of training time\n",
    "total = t1-t0\n",
    "print('Total training time :', total,'Actual training time :', actual_total)\n",
    "print('Win ratio (nb of games won/nb of games played) :', win_cnt/e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for  10 episodes ...\n",
      "0 episodes done. Reward : 186.0\n",
      "1 episodes done. Reward : 200.0\n",
      "2 episodes done. Reward : 178.0\n",
      "3 episodes done. Reward : 165.0\n",
      "4 episodes done. Reward : 162.0\n",
      "5 episodes done. Reward : 196.0\n",
      "6 episodes done. Reward : 200.0\n",
      "7 episodes done. Reward : 200.0\n",
      "8 episodes done. Reward : 164.0\n",
      "9 episodes done. Reward : 200.0\n",
      "The average reward over the test was : 185.1\n"
     ]
    }
   ],
   "source": [
    "nb_e_test=10  #nb of episodes to test\n",
    "total_rew=0   #Total reward over the episodes\n",
    "\n",
    "print('Testing for ',nb_e_test,'episodes ...')\n",
    "\n",
    "for episode in range(nb_e_test):    #set accumulated reward to 0\n",
    "    acc_reward = 0\n",
    "    \n",
    "    input_t = env.reset()\n",
    "    input_t = input_t.reshape((1,observation_shape))\n",
    "    \n",
    "    game_over = False\n",
    "\n",
    "    while not game_over:\n",
    "        \n",
    "        input_tm1 = input_t\n",
    "\n",
    "        q = model.predict(input_tm1)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        input_t, reward, game_over, infodemerde = env.step(action)\n",
    "        input_t = input_t.reshape((1,observation_shape))\n",
    "\n",
    "        acc_reward += reward\n",
    "\n",
    "        if acc_reward>=200:\n",
    "            game_over=True\n",
    "\n",
    "    total_rew+=acc_reward\n",
    "\n",
    "    print(episode,'episodes done. Reward :', acc_reward)\n",
    "\n",
    "print('The average reward over the test was :',total_rew/nb_e_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
