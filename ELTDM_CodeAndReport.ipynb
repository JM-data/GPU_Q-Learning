{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELTDM Project for ENSAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by Jean-Michel Roufosse - Master Data Science @ ENSAE ParisTech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning is very trendy nowadays, especially ever since Google's AlphaGo beat some of the worlds best Go players. How did they manage to teach a computer how to play a game, and more importantly, how to win it ?\n",
    "\n",
    "The research team at Deepmind showed AlphaGo a large number of strong amateur games to help it develop its own understanding of what reasonable human play looks like. Then they had it play against different versions of itself thousands of times, each time learning from its mistakes and incrementally improving until it became immensely strong, and that is what is known as reinforcement learning.\n",
    "\n",
    "We won't be dealing with the game of Go in this project, but we did use OpenAI Gym, a website that provides ready-to-use classic reinforcement learning environments. The OpenAI Gym provides many standard environments for people to test their reinforcement algorithms. These environments include classic games like Atari Breakout and Doom, and simulated physical environments like CartPole, the example used in this project.\n",
    "\n",
    "Below is a part of code that illustrates in Jupyter what the CartPole example game looks like. We need to stabilize the pole for as long as possible, while staying in a certain range. GIF might not be visible through GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script language=\"javascript\">\n",
       "  /* Define the Animation class */\n",
       "  function Animation(frames, img_id, slider_id, interval, loop_select_id){\n",
       "    this.img_id = img_id;\n",
       "    this.slider_id = slider_id;\n",
       "    this.loop_select_id = loop_select_id;\n",
       "    this.interval = interval;\n",
       "    this.current_frame = 0;\n",
       "    this.direction = 0;\n",
       "    this.timer = null;\n",
       "    this.frames = new Array(frames.length);\n",
       "\n",
       "    for (var i=0; i<frames.length; i++)\n",
       "    {\n",
       "     this.frames[i] = new Image();\n",
       "     this.frames[i].src = frames[i];\n",
       "    }\n",
       "    document.getElementById(this.slider_id).max = this.frames.length - 1;\n",
       "    this.set_frame(this.current_frame);\n",
       "  }\n",
       "\n",
       "  Animation.prototype.get_loop_state = function(){\n",
       "    var button_group = document[this.loop_select_id].state;\n",
       "    for (var i = 0; i < button_group.length; i++) {\n",
       "        var button = button_group[i];\n",
       "        if (button.checked) {\n",
       "            return button.value;\n",
       "        }\n",
       "    }\n",
       "    return undefined;\n",
       "  }\n",
       "\n",
       "  Animation.prototype.set_frame = function(frame){\n",
       "    this.current_frame = frame;\n",
       "    document.getElementById(this.img_id).src = this.frames[this.current_frame].src;\n",
       "    document.getElementById(this.slider_id).value = this.current_frame;\n",
       "  }\n",
       "\n",
       "  Animation.prototype.next_frame = function()\n",
       "  {\n",
       "    this.set_frame(Math.min(this.frames.length - 1, this.current_frame + 1));\n",
       "  }\n",
       "\n",
       "  Animation.prototype.previous_frame = function()\n",
       "  {\n",
       "    this.set_frame(Math.max(0, this.current_frame - 1));\n",
       "  }\n",
       "\n",
       "  Animation.prototype.first_frame = function()\n",
       "  {\n",
       "    this.set_frame(0);\n",
       "  }\n",
       "\n",
       "  Animation.prototype.last_frame = function()\n",
       "  {\n",
       "    this.set_frame(this.frames.length - 1);\n",
       "  }\n",
       "\n",
       "  Animation.prototype.slower = function()\n",
       "  {\n",
       "    this.interval /= 0.7;\n",
       "    if(this.direction > 0){this.play_animation();}\n",
       "    else if(this.direction < 0){this.reverse_animation();}\n",
       "  }\n",
       "\n",
       "  Animation.prototype.faster = function()\n",
       "  {\n",
       "    this.interval *= 0.7;\n",
       "    if(this.direction > 0){this.play_animation();}\n",
       "    else if(this.direction < 0){this.reverse_animation();}\n",
       "  }\n",
       "\n",
       "  Animation.prototype.anim_step_forward = function()\n",
       "  {\n",
       "    this.current_frame += 1;\n",
       "    if(this.current_frame < this.frames.length){\n",
       "      this.set_frame(this.current_frame);\n",
       "    }else{\n",
       "      var loop_state = this.get_loop_state();\n",
       "      if(loop_state == \"loop\"){\n",
       "        this.first_frame();\n",
       "      }else if(loop_state == \"reflect\"){\n",
       "        this.last_frame();\n",
       "        this.reverse_animation();\n",
       "      }else{\n",
       "        this.pause_animation();\n",
       "        this.last_frame();\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  Animation.prototype.anim_step_reverse = function()\n",
       "  {\n",
       "    this.current_frame -= 1;\n",
       "    if(this.current_frame >= 0){\n",
       "      this.set_frame(this.current_frame);\n",
       "    }else{\n",
       "      var loop_state = this.get_loop_state();\n",
       "      if(loop_state == \"loop\"){\n",
       "        this.last_frame();\n",
       "      }else if(loop_state == \"reflect\"){\n",
       "        this.first_frame();\n",
       "        this.play_animation();\n",
       "      }else{\n",
       "        this.pause_animation();\n",
       "        this.first_frame();\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  Animation.prototype.pause_animation = function()\n",
       "  {\n",
       "    this.direction = 0;\n",
       "    if (this.timer){\n",
       "      clearInterval(this.timer);\n",
       "      this.timer = null;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  Animation.prototype.play_animation = function()\n",
       "  {\n",
       "    this.pause_animation();\n",
       "    this.direction = 1;\n",
       "    var t = this;\n",
       "    if (!this.timer) this.timer = setInterval(function(){t.anim_step_forward();}, this.interval);\n",
       "  }\n",
       "\n",
       "  Animation.prototype.reverse_animation = function()\n",
       "  {\n",
       "    this.pause_animation();\n",
       "    this.direction = -1;\n",
       "    var t = this;\n",
       "    if (!this.timer) this.timer = setInterval(function(){t.anim_step_reverse();}, this.interval);\n",
       "  }\n",
       "</script>\n",
       "\n",
       "<div class=\"animation\" align=\"center\">\n",
       "    <img id=\"_anim_imgJEUZSWBXWEHPKEWW\">\n",
       "    <br>\n",
       "    <input id=\"_anim_sliderJEUZSWBXWEHPKEWW\" type=\"range\" style=\"width:350px\" name=\"points\" min=\"0\" max=\"1\" step=\"1\" value=\"0\" onchange=\"animJEUZSWBXWEHPKEWW.set_frame(parseInt(this.value));\"></input>\n",
       "    <br>\n",
       "    <button onclick=\"animJEUZSWBXWEHPKEWW.slower()\">&#8211;</button>\n",
       "    <button onclick=\"animJEUZSWBXWEHPKEWW.first_frame()\"><img class=\"anim_icon\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAQAAAAngNWGAAAAAXNSR0IArs4c6QAAAAJiS0dEAP+Hj8y/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAB3RJTUUH3QURCAgaeZk4EQAAASlJREFUKM/dkj9LQnEUhp9zr3bpj1uBcKGiJWxzLWivKAIRjIhcCqcgqJbKRagPICiVSVEuNTu0tLYGUg4tkRGUdxLJ0u79Ndxr5FfwTO/L+xzO4XCgO+v2T70AFU+/A/Dhmlzg6Pr0DKAMwOH4zQxAAbAkv2xNeF2RoQUVc1ytgttXUbWVdN1dOPE8pz4j4APQsdFtKA0WY6vpKjqvVciHnvZTS6Ja4HgggJLs7MHxl9nCh8NYcO+iGG0agiaC4h9oa6Vsw2yiK+QHSZT934YoEQABNBcTNDszsrhm1m1B+bFS86PT6QFppx6oeSaeOwlMXRp1h4aK13Y2kuHhUo9ykPboPvFjeEvsrhTMt3ylHyB0r8KZyYdCrbfj4OveoHMANjuyx+76rV+/blxKMZUnLgAAAABJRU5ErkJggg==\"></button>\n",
       "    <button onclick=\"animJEUZSWBXWEHPKEWW.previous_frame()\"><img class=\"anim_icon\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAQAAAAngNWGAAAAAXNSR0IArs4c6QAAAAJiS0dEAP+Hj8y/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAB3RJTUUH3QURCAgyTCyQ6wAAANRJREFUKM9jYBjO4AiUfgzFGGAp4+yayUvX6jMwMDCsYmBgOCS4OAOrSYmMgcc8/pd5Q3irC+Neh/1AlmeBMVgZmP8yMLD8/c/cqv9r90whzv/MX7Eq/MfAwMDIwCuZdfSV8U8WDgZGRmYGrAoZGRgY/jO8b3sj/J2F6T8j4z80pzEhmIwMjAxsSbqqlkeZGP//Z8SlkJnhPwMjwx/Guoe1NhmRwk+YGH5jV8jOwMPHzcDBysAwh8FrxQwtPU99HrwBXsnAwMDAsJiBgYGBoZ1xmKYqALHhMpn1o7igAAAAAElFTkSuQmCC\"></button>\n",
       "    <button onclick=\"animJEUZSWBXWEHPKEWW.reverse_animation()\"><img class=\"anim_icon\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAQAAAAngNWGAAAAAXNSR0IArs4c6QAAAAJiS0dEAP+Hj8y/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAB3RJTUUH3QURCAgmVvZElgAAAVFJREFUKM+t0k8ow3EYx/H3s/2aLDUSZctFkgsHEi1XLi5ukpPSWsuJklwclsPSsDKFi7MSJ0I5qF2GHO2m0FY7+BdNv7Y9DpuxDSt5vsfvq+fT9/k+8D8VBxIAWH6H0ead4Qb5BRwCENoceZi5Stl/6BgCBmtWhjzxg4mUQ02rAhil7JgB9tze7aTLxFAKsUUd14B9ZzCyFUk401gQyQJaDNcBHwv7t7ETd0ZVQFEEzcNCdE/1wtj15imGWlEB8qkf2QaAWjbG/bPSamIDyX65/iwDIFx7tWjUvWCoSo5oGbYATN7PORt7W9IZEQXJH8ohuN7C0VVX91KNqYhq4a1lEGJI0j892tazXCWQRUpwAbYDcHczPxXuajq3mbnhfANz5eOJxsuNvs7+jud0UcuyL3QAkuEMx4rnIvBYq1JhEwPAUb3fG7x8tVdc292/7Po7f2VqA+Yz7ZwAAAAASUVORK5CYII=\"></button>\n",
       "    <button onclick=\"animJEUZSWBXWEHPKEWW.pause_animation()\"><img class=\"anim_icon\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAQAAAAngNWGAAAAAXNSR0IArs4c6QAAAAJiS0dEAP+Hj8y/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAB3RJTUUH3QURCAkR91DQ2AAAAKtJREFUKM9jYCANTEVib2K4jcRbzQihGWEC00JuNjN8Z2Q0Zo3VYWA4lL005venH9+c3ZK5IfIsMIXMBtc12Bj+MMgxMDAwMPzWe2TBzPCf4SLcZCYY4/9/RgZGBiaYFf8gljFhKiQERhUOeoX/Gf8y/GX4y/APmlj+Mfxj+MfwH64Qnnq0zr9fyfLrPzP3eQYGBobvk5x4GX4xMIij23gdib0cRWYHiVmAAQDK5ircshCbHQAAAABJRU5ErkJggg==\"></button>\n",
       "    <button onclick=\"animJEUZSWBXWEHPKEWW.play_animation()\"><img class=\"anim_icon\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAQAAAAngNWGAAAAAXNSR0IArs4c6QAAAAJiS0dEAP+Hj8y/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAB3RJTUUH3QURCAkEmo00MwAAAS9JREFUKM+tkj1IQmEUhp9j94LQj0FD4RRBLdLQ3ftb26PRcCiQIIiIDFwKC0OhaAiam5wVDBpqCKohQojMLYzaAiUatOtpuQrKVQl64fu+4Xt4OLwc+Fs+nNM16jsPAWS6gZXggoZfXmfhog3hcZ6aTXF87Sp68OmH4/YggAo8bmfyyeh6Z1AAKPVldyO1+Iz2uILq3AriJSe3l+H7aj+cuRnrTsVDxSxay+VYbMDnCtZxxQOU9G4nlU9E1HQBxRkCQMRGRnIbpxMARkvxCIoAorYMMrq0mJ0qu4COUW3xyVDqJC4P+86P0ewDQbQqgevhlc2C8ETApXAEFLzvwa3EXG9BoIE1GQUbv1h7k4fTXxBu6cKgUbX5M3ZzNC+a7rQ936HV56SlRpcle+Mf8wvgJ16zo/4BtQAAAABJRU5ErkJggg==\"></button>\n",
       "    <button onclick=\"animJEUZSWBXWEHPKEWW.next_frame()\"><img class=\"anim_icon\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAQAAAAngNWGAAAAAXNSR0IArs4c6QAAAAJiS0dEAP+Hj8y/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAB3RJTUUH3QURCAkd/uac8wAAAMhJREFUKM9jYBie4DEUQ8B+fEq3+3UrMzAwMFxjYGBgYJizYubaOUxYFUaXh/6vWfRfEMIL/+//P5gZJoei4/f/7wxnY1PeNUXdE2RgYGZgYoCrY2BBVsjKwMDAwvCS4f3SG/dXxm5gYESSQ1HIwvCPgZmB8f8Pxv+Kxxb/YfiPJIdi9T8GJgaG/38ZFd4Fx0xUYsZt4h8GBgb2D2bLy7KnMTAwMEIxFoVCXIYr1IoDnkF4XAysqNIwUMDAwMDAsADKS2NkGL4AAIARMlfNIfZMAAAAAElFTkSuQmCC\"></button>\n",
       "    <button onclick=\"animJEUZSWBXWEHPKEWW.last_frame()\"><img class=\"anim_icon\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAQAAAAngNWGAAAAAXNSR0IArs4c6QAAAAJiS0dEAP+Hj8y/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAB3RJTUUH3QURCAknOOpFQQAAAS9JREFUKM/dkrEvQ3EQxz/33mtoQxiYpANbLU26NAabSCcSUouGBVNDjYQaOiDpIEiKjURIw2Kx04hEYmkHEpGoJpSISaXq9Wd4P03/ht5y98197/u9XA4aK4rAWw3lgWddZ3S+/G9mEovtAB8AHE4pgTQAx8PbJweRmsq6GimmNpxaNYXVzMNNCI6A2figimwCGACK786zuWgh3qcsKf/w0pM4X0m/doNVFVzVGlEQsdRj193VxEWpH0RsdRu+zi3tVMqCAsDShoiYqiSV4OouVDFEqS9Pbiyg7vV62lpQ2BJ4Gg0meg0MbNpkYG/e+540NNFyrE1a8qHk5BaAjfnrzUaHfAWImVrLIXbgnx4/9X06s35cweWsVACa3a24PVp0X+rPv1aHFnSONdiL8Qci0lzwpOM5sQAAAABJRU5ErkJggg==\"></button>\n",
       "    <button onclick=\"animJEUZSWBXWEHPKEWW.faster()\">+</button>\n",
       "  <form action=\"#n\" name=\"_anim_loop_selectJEUZSWBXWEHPKEWW\" class=\"anim_control\">\n",
       "    <input type=\"radio\" name=\"state\" value=\"once\" > Once </input>\n",
       "    <input type=\"radio\" name=\"state\" value=\"loop\" checked> Loop </input>\n",
       "    <input type=\"radio\" name=\"state\" value=\"reflect\" > Reflect </input>\n",
       "  </form>\n",
       "</div>\n",
       "\n",
       "\n",
       "<script language=\"javascript\">\n",
       "  /* Instantiate the Animation class. */\n",
       "  /* The IDs given should match those used in the template above. */\n",
       "  (function() {\n",
       "    var img_id = \"_anim_imgJEUZSWBXWEHPKEWW\";\n",
       "    var slider_id = \"_anim_sliderJEUZSWBXWEHPKEWW\";\n",
       "    var loop_select_id = \"_anim_loop_selectJEUZSWBXWEHPKEWW\";\n",
       "    var frames = new Array(0);\n",
       "    \n",
       "  frames[0] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABMJJREFUeJzt3LFNw1AARdEYZYnMAWNkjmQmPAdrZA7G+BSpgAYC8ueGcyQXcWG9xrr6kpVljDF2ABDzMHsAANxCwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkvazB8B/dVnPn+49np4nLIEmJzCYQLzg5wQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjDY2GU9z54Ad0HAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAGDX7Asy5evnz4DuBIwAJIEDCZ5eT3tXl5Ps2dAloDBxj6GS8TgNgIGf8DTeZ09AXIEDIAkAYONHQ/r7nhY3/0Gvm8ZY4zZI6Buy8/bvbJw5QQGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQNJ+9gC4B/4dA7bnBAZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZA0huyiyZ8ok8ysAAAAABJRU5ErkJggg==\"\n",
       "  frames[1] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABMlJREFUeJzt3O1Jw1AYgFEjXaJz6Bido53JzOEancMx4j8RCmI/yO3TngOBEEh4/4SHewmZlmVZXgAg5nX0AABwCQEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEjajB4AntVxPpxce9t/DJgEmqzAAEgSMBjA6guuJ2AAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGCwsuN8GD0CPAQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwOBGpmn613Ht/X89A56JgAGQtBk9ADyrz6/9z/luOw+cBJoEDFb2O1zA5Wwhwh0QNTifgMEdsIUI5xMwWNluO58E6/0gYHCuaVmWZfQQ8AjW/LzdawtWYABECRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAk+Rs93Ii/Y8C6rMAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASPoGdq4jfUCx97IAAAAASUVORK5CYII=\"\n",
       "  frames[2] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABMZJREFUeJzt3cFpAlEUQNFMsInUYRupQ2vSOtKGdaSM7yIQQpKFGDM/N54DAzogvI1cHjOjyxhjPABAzOPsAQDgGgIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZC0mT0A3KvTcf/l3HZ3mDAJNNnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAGDCTwDBj8nYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAwcpOx/3sEeBfEDAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjD4A7a7w+wRIEfAAEgSMACSBAyAJAGDG1iW5eLjNz4P90jAAEjazB4A7tHL6+799fPTceIk0GUDg5V9jNd374HLCBgASQIGQJKAwco+X/NyDQyus4wxxuwhoG7N29t9ZeGNDQyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJH+nAjfg1zFgfTYwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJLOQnkifIzBpREAAAAASUVORK5CYII=\"\n",
       "  frames[3] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABMZJREFUeJzt3dFJw1AYgFEjXcI5uoZztDO1c7hG53CM2wdBRH0oteb62XMgkAQC/0v4uCQkyxhjPABAzOPsAQDgGgIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZC0mT0A3KvTcf/l3HZ3mDAJNFmBAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYDCBr3DAzwkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYrOx03M8eAf4FAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDP6A7e4wewTIETAAkgQMbmBZlou337ge7pGAAZC0mT0A3KOX1937/vPTceIk0GUFBiv7GK/vjoHLCBgASQIGQJKAwco+P/PyDAyus4wxxuwhoG7N19vdsvDGCgyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJL9TgRvwdQxYnxUYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAElnALgifMGtcugAAAAASUVORK5CYII=\"\n",
       "  frames[4] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABMVJREFUeJzt3Ntpw0AUQMEouAnXkTZch11TXEfacB0pY/MXAoEQP9D62DMgEAKJ+yMOK8QuY4zxAgAxr7MHAIBLCBgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQNJm9gDwrE7Hw69rb/v3CZNAkxUYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBhPYhQOuJ2AAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgMHKTsfD7BHgIQgYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgMGNLMvyr+Pa+/96BjwTAQMgaTN7AHhWH5/77/Pd9jhxEmgSMFjZz3ABl/MJEe6AqMH5BAzugE+IcD4Bg5XttkfBghtYxhhj9hDwCNb8vd1rC1ZgAEQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACTZjR5uxO4YsC4rMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSvgDwzSJIo42vogAAAABJRU5ErkJggg==\"\n",
       "  frames[5] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABMpJREFUeJzt3NFJw1AYgFEjXaJzuEbnaGcyc7hG53CM+CaCIqaW3H7tORAIgYT/JXzcEO60LMvyBAAxz6MHAIBLCBgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQNJu9ADwqM7z6du1l+PrgEmgyQoMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAY4Kd9EIF1BAxuhI18YR0BAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMNnaeT6NHgLsgYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAwZVM0/Sn47/3//YMeCQCBkDSbvQA8Kje3o+f54f9PHASaBIw2NjXcAGX8wkRboCowXoCBjfAJ0RYT8BgY4f9LFhwBdOyLMvoIeAebPl7u9cWrMAAiBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASLIbPVyJ3TFgW1ZgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACR9AMI2IkiSV3RlAAAAAElFTkSuQmCC\"\n",
       "  frames[6] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABMJJREFUeJzt3d1JxEAYQFEj24R12IZ1rDXt1mEZWodljI+KCv7FDHdzDuQhgcD3Ei4TwmQZY4wrAIi5nj0AAPyGgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJB1mDwB79XS+/3Dt9niaMAk0WYEBkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgMIGd6OHvBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAYCXLsnz7+I/7YW8EDDb2eDrOHgEuwmH2ALBXD8+vIbu7OU+cBJqswGCCt/H67Bz4moDBxsQK1iFgsDGvC2EdAgYTvI+YqMHPLWOMMXsIuARbft7usQUrMACiBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACS/E4FVmJ3DNiWFRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASS+r2SRHLKd7IQAAAABJRU5ErkJggg==\"\n",
       "  frames[7] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABLRJREFUeJzt3Mtpw1AURVEruInUkTZSh1NTXIfbSB0u42VqyCTOx5dtrwWaCARnIjYPhLa11toBQMzT9AAA+AkBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABI2k8PgEf0cXz7cu/l8D6wBLqcwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwOCPbNv27es/nodHI2AAJAkYDDqdD7vT+TA9A5IEDIZchkvE4HoCBgMEC35PwGDA6/NxegLkCRgMuYyYoMH1trXWmh4B9+CWn7d7bcEJDIAoAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDICk/fQAuBf+jgG35QQGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQNInLIQgfB3gDOYAAAAASUVORK5CYII=\"\n",
       "  frames[8] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABMlJREFUeJzt3d1NAkEYQFHX0AR12AZ1QE1uHbZBHZYxvhl/MEEljFfOSUjYfSDfy+ZmdsjuMsYYdwAQcz97AAD4CQEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEjazB4AbtFxPXw697B/nDAJdFmBAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAwQSnXl556iWXwNcEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIw+EOO62H2CJAhYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAwQUty3L257e/AbdOwABI2sweAG7Z0/P+9ftuu06cBHoEDCZ5G6/3x0IG53ALEYAkAQMgScBgko97Xrvtah8MvmEZY4zZQ8B/cc2/t7t0uXVWYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkeRo9XJCnY8D1WIEBkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkPQC75gi4g3KLWIAAAAASUVORK5CYII=\"\n",
       "  frames[9] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABNFJREFUeJzt3cFtwkAUQME4oonUQRupA2qCOtIGdaSMzTmJkVBEWB7MSD7gg/Uv6Gm11noZY4wXAIh5nT0AAPyFgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgAGQJGAAJG1mDwDP5nTcr97f7g43ngTarMAASBIwAJIEDIAkAYMbs9cF1yFgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgMMHagb7nPnQJrBMwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDIAkAQMgScAASBIwAJIEDO6IE+nhcgIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGk2x3h9kjQJqAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYHBnTsf97BEgQcAASBIwAJIEDK5sWZaLr/98Bjw6AQMgaTN7AHh2H5+7b7/f346TJoEWKzCY6Ge8zt0DfhMwAJIEDIAkAYOJ1va77IHBZZYxxpg9BDySW7/a7i/Ms7ICAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyDJ51TgypyMAbdhBQZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZA0hewMyV8kqrBSgAAAABJRU5ErkJggg==\"\n",
       "  frames[10] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABOVJREFUeJzt3dFJA0EUQFFXbMI6tAzrSGrSOmzDOixj/BBETIgxGmevngMLyULC+wmXYTc7yxhjXABAzOXsAQDgFAIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZB0NXsA+G+eHrY752429xMmgTYrMACSBAxWYN+qDDhMwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBg1/mqRvwMwQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjCYYN9/wewJBl8jYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGK+JWejiegAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgMEk+/YEA44nYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGK2NPMDiOgAGQJGAAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBhPZUgVOJ2AAJAkYAEkCBkCSgAGQJGAAJAkYAEkCBkCSgMEK2RMMPidgACQJGABJAgZAkoDBmSzLctTx3c8f+g74ywQMgKSr2QMArx6fN2+v764fJk4CDVZgsALv47XvPbBLwGCy263VFpxCwABIEjBYgY/XvFwDg88tY4wxewj4i37z9nY/Y/4jKzAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkmynAmfi6RhwXlZgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACS9AJEzK3y0oJ+UAAAAAElFTkSuQmCC\"\n",
       "  frames[11] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABPhJREFUeJzt3dtNQkEUQFGvsQnrsA3q0DZsQ+qwDK3DMq6/Gl+AwMzWtRI+IIGcH7IzZHJY1nVdLwAg5nL0AABwCAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEi6Gj0A/DfP27sPr93cPgyYBNqcwOCMPosXcBgBgzNy0oLjETAAkgQMgCQBAyBJwABIEjCYgNuJsD8BAyBJwABIEjAAkgQMgCQBAyBJwODMrJOC4xAwAJIEDIAkAQMgScAASBIwGOCzixzWScF+BAyAJAEDIEnAAEgSMACSBAwm4iIH7E7AAEgSMACSBAyAJAEDIEnAAEgSMBjE/4LB7wgYAEkCBkCSgAGQJGAAJAkYTMY6KdiNgAGQJGAAJAkYAEkCBkCSgMFAtnHA4QQMgCQBAyBJwABIEjAAkgQMJmQbB/xMwABIEjAAkgQMgCQBAyBJwGAw2zjgMAIGQJKAAZAkYAAkCRgASQIGk7KNA74nYAAkCRgASQIGQJKAAZAkYAAkCRhMwDop2J+AwYkty7LT4zfv/e4z4K8SMACSrkYPALz3+HL75tl22BwwOwGDSbwPF/ATPyHCxO7vn0aPANMSMJjY5tpPiPAVAYNJbK63ggV7WNZ1XUcPAX/ZOa+3+zrznziBAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZBkGz2cmO0YcBpOYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkvQIO2DHjX7br5wAAAABJRU5ErkJggg==\"\n",
       "  frames[12] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABRNJREFUeJzt3NFtGkEUQNHdyE24jqQM1wE1mTpSRlxHyth8xVYkhwAmzFx8jrSfSO8HXQ37hnXbtm0BgJgvowcAgEsIGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoABkCRgACQJGABJAgZAkoDBIC+H/fJy2I8eA7IeRg8An4lgwfU4gQGQJGAAJAkYDOZnRbiMgAGQJGAAJAkY3NDX3fPoEeBuCBgASQIGQJKAAZAkYDABq/RwPgEDIEnAAEgSMACSBAxuzF0wuA4BAyBJwABIEjCYhFV6OI+AwQDeg8HHCRgASQIGQJKAwUS8B4PTCRgASQIGQJKAAZAkYDCIVXr4GAEDIEnAAEgSMACSBAwm4y4YnEbAAEgSMACSBAyAJAGDgdwFg8sJGABJAgZAkoDBhKzSw78JGABJAgZAkoABkCRgMJhVeriMgAGQJGAAJAkYTMoqPRwnYAAkCRgASQIGQJKAAZAkYDABd8HgfAIGQJKAAZAkYDAxd8Hg7wQMgCQBAyBJwABIEjCYhFV6OI+AAZAkYAAkCRhMzio9vE/AYCLeg8HpBAxuYF3Xk5//8Xm4RwIGQJKAwYS+/9y9PsuyLD+ed4Mngvk8jB4AOO53xJblMHQOmI0TGEzmLVjAMQIGk3l6dNKCUwgYTObb/s+APT0eRA3esW7bto0eAu7dLdfbfaX5LJzAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEjyb/RwA/4dA67PCQyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyAJAEDIEnAAEgSMACSBAyApF8XGkGCvmidRgAAAABJRU5ErkJggg==\"\n",
       "  frames[13] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABSJJREFUeJzt3dFt2lAYgNG6yhKdox2jc5A1skYyR8dI5+gY7luktIYYguz7wTkSEkIY3Rf0yde/YZrnef4CADFf914AAFxCwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgCQBAyBJwABIEjAAkgQMgKSHvRcA9+L3y+Pi698PzxuvBG6DMzAAkgQMgCQBAyBJwGAjrnXBdQkYAEkCBjs7Np0InCZgACQJGABJAgZAkoDBhkwiwvUIGABJAgYDMIkI5xMwAJIEDIAkAQMgScBgYyYR4ToEDIAkAYMdLJ2FmUSE8wgYAEkCBkCSgMFAbCPCegIGQJKAAZAkYLAT94PB5wgYAEkCBkCSgMFgTCLCOgIGQJKAAZAkYLAjk4hwOQEDIEnAAEgSMBiQSUT4mIABkCRgACQJGOzMJCJcRsAASBIwAJIEDAZlEhFOEzAAkgQMgCQBgwGYRITzCRgASQIGQJKAwcBMIsJxAgZAkoABkCRgMAiTiHAeAQMgScAASBIwGMjSNqJJRFgmYAAkCRgASQIGAbYR4X8CBkCSgAGQJGAwGDc0wzoCBkCSgEGEQQ54T8BgQ9M0rXp89vhTnwG3QsAASHrYewHAcb/+HN6e//z2suNKYDzTPM/z3ouAe3HO1t7T0+vCaz9WH++rza2zhQhAkoBByOvz4eM3wZ0QMBjUv9e8XAOD91wDgw1tOd7uq82tcwYGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJK/U4EN+XUMuB5nYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAkCRgASQIGQJKAAZAkYAAk/QXEoEU9ZKMqsAAAAABJRU5ErkJggg==\"\n",
       "\n",
       "\n",
       "    /* set a timeout to make sure all the above elements are created before\n",
       "       the object is initialized. */\n",
       "    setTimeout(function() {\n",
       "        animJEUZSWBXWEHPKEWW = new Animation(frames, img_id, slider_id, 50, loop_select_id);\n",
       "    }, 0);\n",
       "  })()\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Run a demo of the environment\n",
    "observation = env.reset()\n",
    "cum_reward = 0\n",
    "frames = []\n",
    "for t in range(5000):\n",
    "    # Render into buffer. \n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "env.render(close=True)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At every time step, you can observe its position $x$, velocity $\\dot{x}$, angle $\\theta$, and angular velocity $\\dot{\\theta}$. These are the observable states of this world. At any state, the cart only has two possible actions: move to the left or move to the right. In other words, the state-space of the Cart-Pole has four dimensions of continuous values and the action-space has one dimension of two discrete values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behing Reinforcement Learning is therefore to decide of the best action to perform according to the information given by the game's environment. In the CartPole's case, the action is either $+1$ or $-1$, indicating if the cart should go to the right or left. So, in short, there's a constant dialogue between an Agent, usually a human playing but here it will be a computer, and the Environment, here the CartPole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAACmCAIAAAC3PLYxAAAAAXNSR0IArs4c6QAAAARnQU1BAACx\njwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAABsPSURBVHhe7Z1bdJXF+Ydz4Wr1QgFrcS1Xu0gv\n2l6GiyKEtBIqyqmshIOtAm04aVchmKQcirUlAWnAICQgIAElIAHl1AAtWkUgoAiypOGgxQPhqBUh\nkLQ0LVgo/4f9TuY/+9s7OwGS7L2//T4Xe807h3fmm3l/38xskpB0TVEUB5WEogShklCUIFQSrUtt\nbW1lZaUxlHhAJdGSHD9+3KQayM/PT0pKKi4uNrYS86gkWhIEYFINVFVV9ejRI1QqSsyikmgxduzY\nESoJJe5QSbQY7AYqCR+gkvDCUaesrIzghoKCApPbANdlMqWUG0JFRQWZ7A+dO3fmzpCcnJweAA/k\n8+magushJydn+fLlpiAgKiqTSR2a4D8rK0u6UNoMlUQQHPoJSlQhJhGZmZkpaSBSCX17V6Za+/bt\n7T0h7C5BE6Ti5qekpLgKoQs0YIxr10iLHxpiSnMkJ6VKG6CSCIIod78gIhYxJTqBSO3UqZOkgcim\n1OqnsYOTKwmPB/AEPRWQmfUJ1A/rVmklVBJerACIy5KSEjdeiU7iXtKhNEcSYT2QyV4haWpSX9JC\nY26VVkIl4QUlcFiSQzzbhSsJN7hDaY4kSIdKghybqZKIOiqJIJABEWnP+nJwspJo166djd1Q3Nh1\nbwuuJHSXiH1UEkEQf27IWkkIBG5ycrIpC8Api3xJu7HrBrErCTxwQZe0IHcJKyGVRNRRSQSRkZHh\nhqwEKEHPCYrYBTYK91tRKthvnAh3mkvaDWJXEuLBqggockUoPRojAHuI601pbVQSQXCRSElJKSgo\nqKysJBAxCfT09HT7HRQCIIJzc3OlgntAoojw5Uaek5MjOqGUtoQ4e4v9ppUilIOHjRs3jhgxQv4V\nQorwTE3qI0va2ubt27cnIXWU1kYlEQZi1P0aNCyNVWiyoaX5NZW2RCWhKEGoJBQlCJWEogShklCU\nIFQSN8+lS5dOnz7NLXnbtm0bNmxYtmzZvHnzCgsLf//730+ePDk3Nzc7O/tXAcaOHTt+/Pi8vLwp\nU6YUFBQUFRUtWrRo5cqVmzdvfuedd44cOXLu3DnjVIk2KolmQfT/7W9/e+2111544YWnn346Kyvr\n4YcfTk1NzcjIGD169KRJk1DCwoULV6xYgTaohkjefvvtPXv27Avw3nvvvfvuu7t27dq6deuf/vSn\nNWvWvPTSS8XFxcgjJydn+PDhvXv37tat28CBA8eNGzdz5sxVq1bt3r37iy++MN0rbYhKIjz19fWE\nMiH+29/+dsiQIcTr0KFDEcOSJUtef/31Q4cOtfh7/fLlyydPnkQ5CGb27Nloo2/fvg8++CCbTElJ\nyZtvvvn555+bqkpropL4f2pqaoi8Z599dtiwYd27d3/88cfnzp3LK//o0aOmRptz/vx5tpqysjI2\nIhQyYMAATmUVFRUc2EwNpaVRSVxjN5g/fz6nl549e06YMKG8vPzw4cOmLMY4ceLExo0bp06dijwe\neeSROXPm7N2715QpLUSCSuLKlStvvfUWsfXjH/945MiRpaWlBw4cMGVxAncbLiSjRo3icDV9+nSu\nLqZAuTUSThI7d+7k7JGWljZ+/Pj169d/+eWXpiBu4Rb+yiuvcMzjxv/cc8998MEHpkC5KRJFEp9+\n+um8efM4bxA6a9eu5YxuCnwEt3Nu/4MGDWLf27BhAzuhKVBuBP9L4o033pBvbxYsWFBdXW1yfQ2H\nqMmTJ3M1ev755/Wb3BvFt5Kor69fsWJFRkbG2LFjUYXJTSTYNObOnZuamlpYWGh/qUNpEh9K4ty5\nc7wdf/SjHxUUFMTsd0dtxj/+8Y/S0tIePXogDP2XjebgK0nU1NQUFxd36dKFW6Z+c+/yr3/9a9Gi\nRffffz+nx8uXL5tcJRw+kQTLvHjx4m7dus2ZM8cHXyK1EmfOnJkxYwbXqk2bNpksJQQ/SGL9+vV9\n+vSZPn267gzNYf/+/WPGjMnJyUmQLxtulPiWhKzuuHHjDh48aLKU5lFeXs4Jk09jKw3EqyQ4Kc2e\nPZvNYfPmzSZLuUGOHTvG2yQvL48DlclS4lQSO3bsyMjImDVrVn19vclSbpalS5c+9NBD7t/RSXDi\nTxJFRUUDBw7ctWuXsZVbZvfu3f369XvxxReNndjEkyQOHTo0dOjQGTNmXLp0yWQpLcSXX345duzY\ngpD/TyMBiRtJrFmzJjU1VW8OrcozzzzD7eLixYvGTkjiQxJ/+MMfRowY8emnnxpbaTUWLFjwi1/8\nIpH/bSfWJXHmzJknnnhi2rRpxlZan2XLlj366KMJ+zVUTEviwIEDP/nJT1ghYyttRVlZ2bBhwy5c\nuGDsRCJ2JbF9+/auXbu+9tprxlbaltLS0scff/zq1avGThhiVBJco9PT0/UXi6NLUVHRlClTjJEw\nxKIk1q1b169fvw8//NDYSvSYMGHC/PnzjdGAv/9RKOYkgR4GDBgQxb8To7hcvHhxyJAhW7ZsMXbg\ngnfHHXf4+FcvYksSGzdu7N+/v+ohpqiqqvrhD3944sQJMUtKStq1a7d161Yx/UcMSYJZfvDBB48c\nOWJsJWZYuXJldna2pLt27ZqUlIQwxPQfsSKJ999/v0uXLnwaW4kxnnzyyfLy8rNnz3bo0AFJDB48\n2BT4jpiQxKlTp3r37u3jvThO+fa3v23/oGB1dTX7w+LFi++8804k8d3vflfq+I+YkMTIkSPZmo2h\nxAZXr17NzMzkJt2xY8d777134MCBLNP3vvc99AD33HNPXV2dqeovoi+JadOmFRYWGkOJMaZMmcJl\nWmRA4rbbbpM0kti9e7ep5C+iLInVq1ePGTPGGEpMMnPmTAQgSrDcfvvtS5cuNTX8RTQlcfDgwbS0\ntGPHjhlbiVXKysruvvtuo4YGRo0aZYr9RTQlMWzYMP39h3ihoqJCLtaWzp07mzJ/ETVJFBcX6498\nxxfbt2/nnm0EkZR01113ffXVV6bMR0RHEnv37u3Xr1+C//ZWPLJ///7vfOc7IomOHTvG3X/K0Ryi\nIwmOTH/5y1+MocQV1dXVKSkpXK/ZJVatWmVyfUQUJPHSSy899dRTxlDikJqaml69et12220TJkww\nWT4imtdrJX7573//+8ADD3zrW98yto9QSShKECoJRQlCJaEoQagkFCUIlYSiBNGEJKqqqsrKyoyh\nKAlAE5LIyspKTk42hqIkAE1IQn5Wnr3C2G1FbW2t/o8HSlSIJImKioqMjAwkkZOTY7LaCvSQn59v\nDMWBtejRo0fgh4ySUlJSSFs6depEZkvNG2+lzMzMBDw2R5IEpybmhY2i7c9OrKtKIgIIgOg3hgNL\n1lLvL95KdME70dh+oUmRNyEJ+WRqIp+d7H++T7XKykpJCxR5cgTbBNWBpAWctG/fXiURgcYkAbJq\nLQKqsMvkG5p8ZTQqCU5NoicSzH5YR8wXb5HiACwSQUw0k5BrAKWkcYLJOtm//EOmvaLQRI5n6enp\nIgxxRSnHABIQWY2JiUyRMYLRV0lkiCuTaoRGJeG+bPDCa9sYDpxliWBJk5DzlbxXiG9Md5PCiUgF\nqMyKuotHqXUFnlLFQ6gk7HTZSQbWQpYDyPfsxk1CW8/7yDp3exGoyXEg9P1Fp/aYQGloBbDewvbY\n2LApCj2ABJ640UeWI48xGiF8MY5cSbBF4IjXubED0DGZ9mFIuKYEvaQFnNiDqVR2H541djVAqUoi\nAh5JsDSe6WJupQ75lLIczDmvMFuNUtmrSchC8P4iB+REIBX4DFS/vny8tmTVZCnZ2KWIHEyaExJ8\nWodA2m3FGBgJw7CRat2SY4dKBVyFHbZAL8Sn1LHd8Uk6wiMzSEx5ZEHyPYSXBF1mZmYWNDBixAgc\nuSIRyKSmpCXKrUDpr3PnzojYghM7CKksaYEi97Ep9cyC4sJ0yRSBTZsyB4qA2RbTs0asnWsCTmw0\nAytOc2M0NJeOCDgpIpQ5DtgugDRRboOe6KSVyEyg1B2t61bAM3qzPqWCHScO8SBpoBTp2u6uP7Dz\nyNK7LRVXkm6M8MXMBY1dGETo2YnHsK8KJOg+tozMGCHg0DMyKruT4pkjxQPT5U4gARp2uqjGe9EY\nIRs7EExuQ48TTHcRZdUIMmMHoI4boIIb9NLK7dSz1mEreILerUCRG2ngjoq2IGnwtBVT0o0RphhJ\nhW4IDMLtWODByKFIEiY3AB4ifHUbOjIew50mSq3p5isC0+WZQHeWbAQEwiPoxUQrWwq0sstEfuj6\nus1l1dzmgOQ8XUCgW5MprSQtUGSP0BDq1m0ObgVRNS9ic4AJwCPYx4/Q1pqSbowwxeyn1oWF/RRf\nHqmEKscifbubMthxh46Mx3AX1X17ufmKwHR5JtBONQl7mg2ER1C80spdXIkwUULoPJPjNpdV88QG\n72xPF0COfc2HXWu3SajbCBUkDiOERIS21pR0Y3iLaYPm2IiN3QBbB744O9ljGbA/cGHIzc0VsZaU\nlLgaYNy8Qmx9V2n4x5vripquwHiLiOkusGJh1RtbWneePfEBbnwITDWwFjcnCdp6ugBWk3xJSytJ\nC4FBRXIbuQLpFpEEaZvvEjRWQpxApD+exz2uMcuSDySkSyYRGdh86jMR9OdGMF2SLydde2+jue2F\nyvJFgXiwD4Nz6khbyVFcmCi7tB5YBZMKiQ9w40NgiyCTqXZfZwKL4janYWPN3bcbabeatJK04BlV\nqNvIFRiq56+qEUI26iK3FdOmbUy6hJ/W5kDIhsYrI4twhVBaBGKOmGBp3UAE1jgzM9O9mCIPN3ok\nfD0XBqCJfam7EFvNaU6MEgzGCAQGWjJGICQ8Q21yVJEr4IoKVgOYbu9NtuVMLqY7SJeblwSTaHuy\n8KZxl0RpcQgF1jICEisshLEDyPnTGA11LLza7HtU8DQnx9OvVLPQ3BTk57uuTFYAPDQ5quYPW44e\nkil6a2ZbdgYRbdgtAm5eEnLgoWNjB8aEQN1xK0rccfOSANRpxQqkXYUoSjxyS5JQFP+hklCUIFQS\nihKESkJRglBJKDfP888/b1I+QiWh3CTV1dXdu3c/e/assf2CSkK5SSZOnPjyyy8bw0dEQRJffPHF\nfffdd+TIEWMrcciWLVvcH6PwE9HZJe4N4PkhAiVeqKur69279/vvv29sfxEdSTzyyCNJgR81f/XV\nV02WEj/k5+fPmzfPGL4jOpIoLS39+te/jirYK2bOnGlylXhg06ZNw4cPN4YfiY4k9u3b941vfANJ\nyF6Rl5dnCpTY5tixY2lpaYcOHTK2H4mOJL766qu77rpLJAF33nlnRkbGv//9b1OsxCpjxoxZvXq1\nMXxKdCQBqampRhABbr/99q5du/ISMsVK7DFr1qyCggJj+JeoSWLixIlGDQ7Jycl79uwxNZRYgs0h\nKyvr6tWrxvYvUZPE+vXrOS8ZKTjY3wNUYoft27f36tXrxIkTxvY1UZNEdXX1N7/5TZHBHXfc8f3v\nf79Dhw4dO3YUVSxYsMDUU6LNgQMHOOXu3bvX2H4napKA++67TyTxxBNPDBo0qK6ubtu2bTNnzrz/\n/vu/9rWvvfnmm6aeEj2OHj3ap0+f119/3dgJQDQlMWDAAPRwzz337Ny5s7CwsKioyBRcu/bPf/7T\npJTocerUqYEDB65bt87YiUE0JVEc+ANnd99995UrVy5fvjx48OA33njDlCnR5vTp00OGDFm1apWx\nE4ZoSmJH4G8+9+vXT8x9+/alp6efPHlSTCWKcNNjfygvLzd2IhFNSdTW1nKxXrhwobGvXWMNRo0a\nZQwlShw8eLBv375r1641doIRTUlAhw4dNmzYYIwAXK9/97vfGUNpcyorK7t3775lyxZjJx5RlkRY\n8vLy5s6dawylDWFn6Nmz57vvvmvshCQWJcFVe8yYMS+88IKxlTahuLj4scceO3r0qLETlViUBNTV\n1WVlZZWWlhpbaU1qampyc3N/85vf6E9eQoxKAi5cuMBV28e/qhIj7N69u3///vr2scSuJKC+vj47\nO3v69OnGVlqaxYsX9+7dW3/j1yWmJSHk5+cjDDZ3YystwSeffPLLX/5y0qRJ586dM1lKgDiQBCxZ\nsmTAgAHvvfeesZVbY8WKFV27dtVffA9LfEgCtm7d2rNnz+XLlxtbuSmqqqpGjx6dl5eXID/pfRPE\njSTgs88+Gz9+fG5u7unTp02W0mzq6uqKioq4OWzatMlkKeGIJ0kIZWVl3bt3X7NmjbGVZlBeXp6e\nnj5nzpz6+nqTpTRC/EkCPvroo7EBDh8+bLKURti8efPgwYMnT5788ccfmywlInEpCWH9+vW9evXi\nMHD+/HmTpTj8+c9/fuyxx8aNG6dfS9wQcSwJuHjxYnFxcVpa2tKlSy9fvmxyEx5eFj/96U/ZRd95\n5x2TpTSb+JaEcPz48WnTprFjLFu2LJF/JOHcuXNLlizp06fPxIkT9+3bZ3KVG8QPkhA++eQThPHA\nAw/Mnz//888/N7mJAQKYOnVqt27dZs2apXeGW8Q/khBOnTrFUQphPPXUU77/Iee///3vy5cvf/TR\nR4cOHbp69WqOkaZAuQX8JgmBe8W6deuysrIGDRrEWcJn/xv3hQsX/vjHP2ZnZ6enpxcWFv71r381\nBUpL4E9JWA4fPjxnzhyO16NHj165ciV7iCmIQxj8K6+8Mm7cuLS0tKeffvqtt94yBUqL4nNJWDhE\n8UJFG8OHD1+4cOH+/ftNQWxTX1+/a9euuXPn/uxnP+vbt+8zzzyzffv2K1eumGKlFUgUSVg4Zixa\ntIgzFfeNvLy8l19++eDBg//73/9McQxw5syZbdu2lZSUjBw5khvz2LFjly1b9sEHH5hipZVJOElY\nOJFv3bp19uzZP//5z4m8UaNGkd60adOHH3546dIlU6lNOHHiBBooLS2dMGFC//79H374YbSKDNjK\nEuHPEscaiSsJl//85z/E36pVq/Lz84cNG4ZCMjMzn3zyyaKiIjI5q6CTs2fP3kqAshHV1NR89NFH\nO3fuXLt2LZvApEmTOA6lpqbS169//Wv2Lq4Hn332mWmgRAmVRHhOnjz59ttvc5197rnnJk6cyE7C\nPaRLly69evUaPHgwR5rs7OzJkydPnTp1xowZzz77LDsMNfmcNWsWOeRTSh1qDhky5KGHHvrBD37A\n63/o0KG5ubnUWbFiBQL4+OOP23hHUppEJXFj1NbWHj9+/NChQ3v27OG0s2XLloqKCt76r776Kvrh\nc926deSQTyl3empS//z58zF1XVEioJJQlCBUEooShEpCUYJQSShKENGUxI4dOyorK/WPCPmGqqoq\nFhTi+ofKoimJ/Pz8lJSUHj16GDuuYNXT09OLi4uNrQR+LT4jIyMpKSmuX3NRPjihiriQBIttUg3w\nRmTtiQBjKwF4U6gkbol4kUROTo5JOaCK2tpaYygNqCRuibiQBG++OD3dRQWVRNPwKl2+fHlBQcHG\njRtNVgNWEnIzC72WkSNtmWXPW1mKgHzJodS93kkTKbKEvtclh5olJSV484yB/M4BxLN1iHPXtFRU\nVDBa/LgdUU2aiyltQx82MtQXJ3iGUA+4pWvyje08rDSRNEja5thEY5PgPqw8i+RbJFNaqSSagFM4\nQS+TxUxxn5aZFeSGzSeRxMLI/cyuB5kUSZq27dq1s3PNvTYrK4smQELO+jSkPtX4BJq3b9+eT1rJ\ntY++rAfJ4UREKz4lnzSjtTcHcUhOp06dxKctIk1H7u7Bc2GG9SPPRXfUoSGfjAqfdjDNgcqMEycy\nLcwAT0dHFLld88ljMi2kZZDuY2JSKgPjE5NhiOmZBPebA/zQSp6I3klTWYromkcjk1ZSSnc39Fyx\nRutKgvligmR5BCaLNbA5zLWnAivBK1nSrAEeJA1UtqvOWtpWJHAiwQG0Sk5OxiSf1bIe6NcuJOAE\nhySoKUEj+bKo1htQDZ/GcCDT5tMXw6atmEAOg7TBQQK30qPA2MAYzQYn0qnEokyCDWuBdwRIWpQj\naXCLgPHYJ8WzLZLReiaBWbXikQQVcO6ukUqiCVgzGzQWpsy+gZhoTElbbAXaZmZm2hlnASQCeEt5\ngomaNtpIU0HSLlRwg4MuxBsQCnYVyfQsKg1DnwLItPl4o5V1KFwP+YZxSpC5gduY28jgxBU24NPT\nNTn2SeVxrFZ5UoQqaXBduZMAtPJMgqcX4Ok8jyCP6TaMO1pXEsxO6Kpfj6OGTJloSVvIkdcVYmD9\nMFlgVxtSgXm3YNpoc/27oCga2mp0LQmBUo7RZMqQ8GkKmicJgoNWkraIK0njsJluI4MTGhojAGEt\nni08o+s5IMzryiSgaWu3FObTzoYQeRLss1hYHVkpC008DeOOKEhCTqWSDjvR5MgSAqvIsrHqLCT5\nogoSNJQKoeA8tFMBt+np6STwaQUG+GdUdiHx74kG65CgkQS4HeE59EHcpwuNFddt8wl9doIytGsX\nHpYKMpOMn4eV6cWP+9bHjDwJob2Qo5K4MdygsTBldlEbm2ip4G7rwEJKTujBCezqhu1UsPcE17OM\nwY11TBaVHMmkgnVoRw5uR2EfhEHaI1xorLhumw9O3DFA2DObC0VUQA/SkIeS+q4fkY08r4DpmQRy\npMjivt2E0MeMO1pXEoSgew8GJoscO/WhE+028bS1ixrqFof2fhII1EZDjd2GSMWVsUPq20UVyMGz\nvfG7YeQ2ZAAMSepbyLGHeIo8sYIrt192rREjRrh7V1hw4o4BmAfP9Ro8L29MHsFOkbxT7NiACtye\njdHwvcj15w9ATuhKAZ1ypnUXAp+ex4w7WlcSwPuY6ZZZI24IAnfxmGj77gepYCeUybVLiwfSVkuu\nWymSRGVlJWsP9mtyD/TokZOsosQi+XhmDFQjgMQDnzShgpRKNbej614CQU9DaUIF4ts+F205o9NL\nQUGB+KEVRziikAQmdSTmQnc/C56pTB0821YCPlGFnTdm2A13kGe0TXg06kta8EyCrAufuCITz3RK\nBfqVOhZZCEkzQtJUk8eUzLij1SUBTCiTK0jEWChiAchk6oGlcldaTFqRgNC2FEkryaFCoBODDREX\ncWiMBqjJ0uIHbzIeT3PSLDYVZHiejqQOiHPBbY5bkxsYsKe5PJe0tSoKBYdSX5BWFpozPPJDJ0qw\nswRS2RgNWP8yCUBapCX5AqVS30JDvAFFKMFqyRTHG20hCaX5hEaq0saoJGII3u7x+3L1DSqJGILz\nhkkp0UMlESvI8d0YSvRQSShKECoJRQlCJaEoQagkFCUIlYSiBKGSUJQgVBKKEoRKQlGCUEkoisO1\na/8HIs7nJ/1gpXwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image('Agent-Env-Loop.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the notion of reward. It's crucial because the Agent's role is to maximise that reward, since it's highly correlated to chance of winning the game. Here the reward will be a number between 0 and 200, 200 meaning the game was won because the pole successfully didn't fall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must first illustrate the math behind Q-Learning in order to program it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like explained in the paper, we consider the standard reinforcement learning setting where an agent interacts with an environment $E$ over a number of discrete time steps. At each time step $t$, the agent receives a state st and selects an action at from some set of possible actions $A$ according to its policy $\\pi$, where $\\pi$ is a mapping from states $s_t$ to actions at. In return, the agent receives the next state $s_{t+1}$ and receives a scalar reward $r_t$. The process continues until the agent reaches a terminal state after which the process restarts. The return $R_t = \\sum^{\\infty}_{k=0} \\gamma^k r_{t+k}$ is the total accumulated return from time step $t$ with discount factor $\\gamma \\in [0, 1]$. The goal of the agent is to maximize the expected return from each state $s_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action value $Q^{\\pi}(s, a) = \\mathbb{E}[R_t|s_t = s, a]$ is the expected return for selecting action $a$ in state $s$ and following policy $\\pi$. The optimal value function $Q^{}(s, a) = \\max_{\\pi} Q^{\\pi}(s, a)$ gives the maximum action value for state $s$ and action $a$ achievable by any policy. Similarly, the value of state $s$ under policy $\\pi$ is defined as $V^{\\pi}(s) = \\mathbb{E}[R_t|s_t = s]$ and is simply the expected return for following policy $\\pi$ from state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this said, Q-learning is the method that aims to directly approximate the optimal action value function: $Q^{*}(s, a) \\approx Q(s, a; )$. In one-step Q-learning, the parameters $\\theta$ of the action value function $Q(s, a; \\theta) $ are learned by iteratively minimizing a sequence of loss functions, there the *i*th loss function defined as: $$L_i(\\theta_i) =  \\mathbb{E} \\left (r + \\gamma \\max_{a'} Q(s',a';\\theta_{i-1})-Q(s,a;\\theta_i) \\right ) ^2$$ \n",
    "\n",
    "where $s'$ is the state encountered after state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal will be to implement these functions using a Deep Neural Network and compare results using GPU or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My computer is equiped with a NVIDIA GeForce 740M GPU. Since the GPU is NVIDIA, CUDA and its library for deep neural networks cuDNN are used. cuDNN is an NVIDIA library with functionality used by deep neural network. By default, Theano will detect if it can use cuDNN. If so, it will use it. Finally we configured Theano to run on GPU with CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Theano, we use one hidden layer with 200 hidden units, RELU as the Activation function, and Mean-Squared Error as the Loss function. We had to manually define the hidden layers, then stack them. For the back-propagation of the error term, we compute the gradient of the loss with respect to the weights of each layer.\n",
    "\n",
    "*Code in Theano and Keras being similar for the Q-learning, only the Keras part is commented.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import timeit\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import keras\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "from keras.models import model_from_json\n",
    "import time\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Definition of the Neural Network with Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    X[np.where(X < 0)] = 0\n",
    "    return(X)\n",
    "\n",
    "class Hidden_Layer(object):\n",
    "    def __init__(self, input, n_in, n_out, W=None, b=None):\n",
    "\n",
    "        self.input = input\n",
    "        if W is None:\n",
    "            W_values = np.asarray(\n",
    "                np.random.uniform(low=-0.1,high=0.1,size=(n_in,  n_out)),\n",
    "                dtype=theano.config.floatX)\n",
    "            W_h = theano.shared(value = W_values, name='W_h', borrow=True)\n",
    "            \n",
    "        if b is None:\n",
    "            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b_h = theano.shared(value=b_values, name='b_h', borrow=True)\n",
    "\n",
    "        self.W_h = W_h\n",
    "        self.b_h = b_h\n",
    "        \n",
    "        self.params = [self.W_h, self.b_h] # Parameters of the model\n",
    "        \n",
    "        self.output = T.nnet.relu(T.dot(input, self.W_h) + self.b_h)\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "class Output_Layer(object):\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \n",
    " \n",
    "        self.W = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        self.y_pred = T.dot(input, self.W) + self.b\n",
    "\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "    def mse(self, y):\n",
    "        return T.mean((self.y_pred - y) ** 2) \n",
    "       \n",
    "class Neural_Network(object):\n",
    "    \n",
    "    def __init__(self, input, n_in, n_hidden, n_out):\n",
    "\n",
    "        self.hiddenLayer = Hidden_Layer(\n",
    "            input=input,\n",
    "            n_in= n_in,\n",
    "            n_out=n_hidden,\n",
    "        )\n",
    "\n",
    "        self.output_layer = Output_Layer(\n",
    "            input=self.hiddenLayer.output,\n",
    "            n_in=n_hidden,\n",
    "            n_out=n_out\n",
    "        )\n",
    "\n",
    "\n",
    "        self.mse = (\n",
    "            self.output_layer.mse\n",
    "        )\n",
    "\n",
    "        self.params = self.hiddenLayer.params + self.output_layer.params\n",
    "        \n",
    "        self.input = input\n",
    "        \n",
    "def train_on_batch(dataset_X, dataset_y, classifier, x,y,index,learning_rate=0.01, batch_size=1, n_hidden=500):\n",
    "\n",
    "    train_set_x = theano.shared(np.asarray(dataset_X,\n",
    "                                            dtype=theano.config.floatX),\n",
    "                                borrow=True)\n",
    "    train_set_y = theano.shared(np.asarray(dataset_y,\n",
    "                                            dtype=theano.config.floatX),\n",
    "                                borrow=True)\n",
    "\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "\n",
    "\n",
    "    cost = (\n",
    "        classifier.mse(y)\n",
    "    )\n",
    "\n",
    "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "    \n",
    "    updates = [\n",
    "        (param, param - learning_rate * gparam)\n",
    "        for param, gparam in zip(classifier.params, gparams)\n",
    "    ]\n",
    "\n",
    "\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "  \n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    error = []\n",
    "    error_min = np.inf\n",
    "    for minibatch_index in range(n_train_batches):\n",
    "        minibatch_avg_cost = train_model(minibatch_index)\n",
    "        linear_1 = np.dot(dataset_X, classifier.hiddenLayer.W_h.eval()) +  classifier.hiddenLayer.b_h.eval()\n",
    "        h1 = relu(linear_1)\n",
    "        output_nn = np.dot(h1,classifier.output_layer.W.eval()) + classifier.output_layer.b.eval()\n",
    "        error.append(np.mean((output_nn - dataset_y)**2))\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    \n",
    "    return(error, classifier.hiddenLayer.W_h.eval(), classifier.hiddenLayer.b_h.eval(), classifier.output_layer.W.eval(), classifier.output_layer.b.eval())\n",
    "\n",
    "def predict_NN(X_input, W1, b1, Wo, bo):\n",
    "    return(np.dot(relu(np.dot(X_input,W1) + b1),Wo) + bo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_Theano(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        \n",
    "        \"\"\"Define max length of memory and gamma\"\"\"\n",
    "         \n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remind(self, states, game_over):\n",
    "\n",
    "        \"\"\"Add experience to memory\"\"\"\n",
    "        \n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, W1, b1, Wo, bo, batch_size=10):\n",
    "        \n",
    "        \"\"\"Get the batch input and targets we will train on\"\"\"\n",
    "        \n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = 2\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        \n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        \n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,\n",
    "                                                  size=inputs.shape[0])):\n",
    "            \n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "            inputs[i:i+1] = state_t\n",
    "            targets[i] = predict_NN(state_t, W1, b1, Wo, bo)\n",
    "            Q_sa = np.max(targets[i])\n",
    "            if game_over:\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CartPole on OpenAI Gym and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters : epsilon : 0.1 C : 50 , learning rate : 0.1 batch size for training : 50\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "learning_rate=0.1\n",
    "epsilon = .1\n",
    "num_actions = env.action_space.n \n",
    "max_memory = 4000000000\n",
    "hidden_size = 200\n",
    "batch_size = 50\n",
    "acc_reward=0\n",
    "observation_shape = env.observation_space.shape[0]\n",
    "time_step=0\n",
    "max_time_steps=3000\n",
    "everyC=50\n",
    "agent = Agent_Theano(max_memory=max_memory)\n",
    "win_cnt = 0\n",
    "actual_total=0\n",
    "e=0\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "print('Parameters :','epsilon :', epsilon,'C :', everyC,', learning rate :',\n",
    "      learning_rate, 'batch size for training :', batch_size)\n",
    "\n",
    "N = 6\n",
    "X = np.random.uniform(low=-5.,high=5.,size=(N, 4)).astype('float32')\n",
    "W = np.random.uniform(low=-5.,high=5.,size=(4, 2)).astype('float32')\n",
    "b = np.random.uniform(low=-5.,high=5.,size=2).astype('float32') \n",
    "\n",
    "noise = np.random.normal(0,1,(N,2))\n",
    "\n",
    "y = np.dot(X**2,W) + 5*np.dot(X,W) +  b + noise\n",
    "y=y.astype('float32')\n",
    "\n",
    "\n",
    "# Allocating variables for the data\n",
    "index = T.lscalar()  \n",
    "x = T.matrix('x') \n",
    "y_g = T.matrix('y')  \n",
    "# Neural Net Classifier\n",
    "classifier = Neural_Network(input=x,n_in=4,n_hidden=hidden_size,n_out=2)\n",
    "\n",
    "#Initializing a random backpropagation\n",
    "err, W1, b1, Wo, bo = train_on_batch(X, y, classifier, x=x,y=y_g,index=index, learning_rate=0.0003, batch_size=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for  3000 time-steps ...\n",
      "8 time steps done,  1 episodes done. Reward : 8.0 , loss : 0.29904463332082354\n",
      "17 time steps done,  2 episodes done. Reward : 9.0 , loss : 0.4951476437339578\n",
      "26 time steps done,  3 episodes done. Reward : 9.0 , loss : 0.5425996883471856\n",
      "34 time steps done,  4 episodes done. Reward : 8.0 , loss : 1.1527248978544737\n",
      "43 time steps done,  5 episodes done. Reward : 9.0 , loss : 0.4160094063703441\n",
      "52 time steps done,  6 episodes done. Reward : 9.0 , loss : 1.1516658439908567\n",
      "61 time steps done,  7 episodes done. Reward : 9.0 , loss : 0.5249707605703775\n",
      "71 time steps done,  8 episodes done. Reward : 10.0 , loss : 0.12011496624194067\n",
      "81 time steps done,  9 episodes done. Reward : 10.0 , loss : 0.27881161466624715\n",
      "91 time steps done,  10 episodes done. Reward : 10.0 , loss : 0.29061886339871557\n",
      "101 time steps done,  11 episodes done. Reward : 10.0 , loss : 0.36768737058025996\n",
      "111 time steps done,  12 episodes done. Reward : 10.0 , loss : 0.7924767117469483\n",
      "120 time steps done,  13 episodes done. Reward : 9.0 , loss : 1.3290267149189878\n",
      "130 time steps done,  14 episodes done. Reward : 10.0 , loss : 0.4086283450180639\n",
      "139 time steps done,  15 episodes done. Reward : 9.0 , loss : 0.15830164708448258\n",
      "149 time steps done,  16 episodes done. Reward : 10.0 , loss : 0.4172300248625904\n",
      "163 time steps done,  17 episodes done. Reward : 14.0 , loss : 0.14349826240922514\n",
      "172 time steps done,  18 episodes done. Reward : 9.0 , loss : 1.0434129312125493\n",
      "180 time steps done,  19 episodes done. Reward : 8.0 , loss : 0.2962158768352711\n",
      "191 time steps done,  20 episodes done. Reward : 11.0 , loss : 1.0032677768919183\n",
      "201 time steps done,  21 episodes done. Reward : 10.0 , loss : 0.16297591317408305\n",
      "210 time steps done,  22 episodes done. Reward : 9.0 , loss : 0.35135193869431997\n",
      "222 time steps done,  23 episodes done. Reward : 12.0 , loss : 0.4048418643271124\n",
      "235 time steps done,  24 episodes done. Reward : 13.0 , loss : 0.2386310705204244\n",
      "243 time steps done,  25 episodes done. Reward : 8.0 , loss : 0.8558752657444046\n",
      "251 time steps done,  26 episodes done. Reward : 8.0 , loss : 0.1565896799157055\n",
      "259 time steps done,  27 episodes done. Reward : 8.0 , loss : 0.15083998238352844\n",
      "268 time steps done,  28 episodes done. Reward : 9.0 , loss : 0.24441024207862705\n",
      "278 time steps done,  29 episodes done. Reward : 10.0 , loss : 0.4157686358607823\n",
      "288 time steps done,  30 episodes done. Reward : 10.0 , loss : 0.4568039834409444\n",
      "299 time steps done,  31 episodes done. Reward : 11.0 , loss : 0.33263580964900763\n",
      "308 time steps done,  32 episodes done. Reward : 9.0 , loss : 0.4270767466055309\n",
      "318 time steps done,  33 episodes done. Reward : 10.0 , loss : 0.15803140605936172\n",
      "327 time steps done,  34 episodes done. Reward : 9.0 , loss : 0.4300773582665318\n",
      "337 time steps done,  35 episodes done. Reward : 10.0 , loss : 0.20081961749962385\n",
      "349 time steps done,  36 episodes done. Reward : 12.0 , loss : 0.3551965941918304\n",
      "358 time steps done,  37 episodes done. Reward : 9.0 , loss : 0.18438446794668237\n",
      "369 time steps done,  38 episodes done. Reward : 11.0 , loss : 0.19104871116429073\n",
      "381 time steps done,  39 episodes done. Reward : 12.0 , loss : 0.48264508948125234\n",
      "391 time steps done,  40 episodes done. Reward : 10.0 , loss : 0.4330001413263432\n",
      "401 time steps done,  41 episodes done. Reward : 10.0 , loss : 0.2956426574233289\n",
      "410 time steps done,  42 episodes done. Reward : 9.0 , loss : 2.697518394072119\n",
      "421 time steps done,  43 episodes done. Reward : 11.0 , loss : 0.2817353531544099\n",
      "432 time steps done,  44 episodes done. Reward : 11.0 , loss : 2.7740757730216\n",
      "441 time steps done,  45 episodes done. Reward : 9.0 , loss : 0.6352224035998688\n",
      "451 time steps done,  46 episodes done. Reward : 10.0 , loss : 0.25742215347502734\n",
      "462 time steps done,  47 episodes done. Reward : 11.0 , loss : 0.5320307518028716\n",
      "473 time steps done,  48 episodes done. Reward : 11.0 , loss : 1.05440753905597\n",
      "482 time steps done,  49 episodes done. Reward : 9.0 , loss : 0.8483229360015404\n",
      "491 time steps done,  50 episodes done. Reward : 9.0 , loss : 1.0686219100128846\n",
      "505 time steps done,  51 episodes done. Reward : 14.0 , loss : 0.3906561683680236\n",
      "516 time steps done,  52 episodes done. Reward : 11.0 , loss : 1.1280006148674995\n",
      "526 time steps done,  53 episodes done. Reward : 10.0 , loss : 0.35669489865003096\n",
      "536 time steps done,  54 episodes done. Reward : 10.0 , loss : 1.9702185564376442\n",
      "545 time steps done,  55 episodes done. Reward : 9.0 , loss : 0.5917348500801843\n",
      "554 time steps done,  56 episodes done. Reward : 9.0 , loss : 0.28474568094561814\n",
      "564 time steps done,  57 episodes done. Reward : 10.0 , loss : 0.5638236692270945\n",
      "574 time steps done,  58 episodes done. Reward : 10.0 , loss : 2.321051620790314\n",
      "584 time steps done,  59 episodes done. Reward : 10.0 , loss : 0.44797321215466746\n",
      "593 time steps done,  60 episodes done. Reward : 9.0 , loss : 0.24044002340361345\n",
      "602 time steps done,  61 episodes done. Reward : 9.0 , loss : 0.6768536882670405\n",
      "613 time steps done,  62 episodes done. Reward : 11.0 , loss : 1.0240924268526554\n",
      "622 time steps done,  63 episodes done. Reward : 9.0 , loss : 0.5076133417530354\n",
      "632 time steps done,  64 episodes done. Reward : 10.0 , loss : 0.3010119577049746\n",
      "640 time steps done,  65 episodes done. Reward : 8.0 , loss : 0.7740488048841092\n",
      "648 time steps done,  66 episodes done. Reward : 8.0 , loss : 0.3676459214104913\n",
      "659 time steps done,  67 episodes done. Reward : 11.0 , loss : 0.22985997176224718\n",
      "669 time steps done,  68 episodes done. Reward : 10.0 , loss : 0.4635734238696651\n",
      "681 time steps done,  69 episodes done. Reward : 12.0 , loss : 1.2038315471408154\n",
      "696 time steps done,  70 episodes done. Reward : 15.0 , loss : 1.1744236752859951\n",
      "709 time steps done,  71 episodes done. Reward : 13.0 , loss : 0.1865821188922546\n",
      "738 time steps done,  72 episodes done. Reward : 29.0 , loss : 0.5129153869886806\n",
      "747 time steps done,  73 episodes done. Reward : 9.0 , loss : 0.36672343510438776\n",
      "767 time steps done,  74 episodes done. Reward : 20.0 , loss : 0.19253796009761273\n",
      "802 time steps done,  75 episodes done. Reward : 35.0 , loss : 0.19276213726664593\n",
      "820 time steps done,  76 episodes done. Reward : 18.0 , loss : 0.26250803458340843\n",
      "830 time steps done,  77 episodes done. Reward : 10.0 , loss : 0.24274145899380423\n",
      "845 time steps done,  78 episodes done. Reward : 15.0 , loss : 0.22061978141846747\n",
      "873 time steps done,  79 episodes done. Reward : 28.0 , loss : 0.9494977913990715\n",
      "901 time steps done,  80 episodes done. Reward : 28.0 , loss : 0.5015240798219588\n",
      "922 time steps done,  81 episodes done. Reward : 21.0 , loss : 0.25015926228391216\n",
      "931 time steps done,  82 episodes done. Reward : 9.0 , loss : 3.1470451137019717\n",
      "946 time steps done,  83 episodes done. Reward : 15.0 , loss : 0.23907872978805644\n",
      "1015 time steps done,  84 episodes done. Reward : 69.0 , loss : 0.36514819459169884\n",
      "1043 time steps done,  85 episodes done. Reward : 28.0 , loss : 0.1747402673798141\n",
      "1062 time steps done,  86 episodes done. Reward : 19.0 , loss : 0.235150521297425\n",
      "1108 time steps done,  87 episodes done. Reward : 46.0 , loss : 0.2497411684124011\n",
      "1126 time steps done,  88 episodes done. Reward : 18.0 , loss : 0.12504123686630966\n",
      "1135 time steps done,  89 episodes done. Reward : 9.0 , loss : 0.9882273111740872\n",
      "1154 time steps done,  90 episodes done. Reward : 19.0 , loss : 0.6715375686281494\n",
      "1163 time steps done,  91 episodes done. Reward : 9.0 , loss : 0.3046281658693518\n",
      "1178 time steps done,  92 episodes done. Reward : 15.0 , loss : 0.2658120674910359\n",
      "1188 time steps done,  93 episodes done. Reward : 10.0 , loss : 0.2405791049408365\n",
      "1202 time steps done,  94 episodes done. Reward : 14.0 , loss : 0.29592406322995835\n",
      "1211 time steps done,  95 episodes done. Reward : 9.0 , loss : 0.2779139390229412\n",
      "1220 time steps done,  96 episodes done. Reward : 9.0 , loss : 0.3728781146063443\n",
      "1230 time steps done,  97 episodes done. Reward : 10.0 , loss : 0.3486020292087018\n",
      "1241 time steps done,  98 episodes done. Reward : 11.0 , loss : 0.8578692778880165\n",
      "1254 time steps done,  99 episodes done. Reward : 13.0 , loss : 4.8772492677440304\n",
      "1264 time steps done,  100 episodes done. Reward : 10.0 , loss : 1.0421218252625062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275 time steps done,  101 episodes done. Reward : 11.0 , loss : 2.158126045097474\n",
      "1284 time steps done,  102 episodes done. Reward : 9.0 , loss : 0.3860300078793304\n",
      "1294 time steps done,  103 episodes done. Reward : 10.0 , loss : 0.39554290161116207\n",
      "1303 time steps done,  104 episodes done. Reward : 9.0 , loss : 0.5705787734733492\n",
      "1311 time steps done,  105 episodes done. Reward : 8.0 , loss : 0.17415552386450847\n",
      "1320 time steps done,  106 episodes done. Reward : 9.0 , loss : 0.9537835115795916\n",
      "1330 time steps done,  107 episodes done. Reward : 10.0 , loss : 0.14061360223989885\n",
      "1339 time steps done,  108 episodes done. Reward : 9.0 , loss : 0.28426421614988656\n",
      "1350 time steps done,  109 episodes done. Reward : 11.0 , loss : 0.4952682363273228\n",
      "1361 time steps done,  110 episodes done. Reward : 11.0 , loss : 0.3896040123801635\n",
      "1371 time steps done,  111 episodes done. Reward : 10.0 , loss : 0.2379204805515699\n",
      "1381 time steps done,  112 episodes done. Reward : 10.0 , loss : 0.4050472293581439\n",
      "1390 time steps done,  113 episodes done. Reward : 9.0 , loss : 0.25090392990893834\n",
      "1402 time steps done,  114 episodes done. Reward : 12.0 , loss : 2.2107755655498385\n",
      "1416 time steps done,  115 episodes done. Reward : 14.0 , loss : 0.3433551497064224\n",
      "1426 time steps done,  116 episodes done. Reward : 10.0 , loss : 0.4575147344624997\n",
      "1441 time steps done,  117 episodes done. Reward : 15.0 , loss : 1.195290967527908\n",
      "1450 time steps done,  118 episodes done. Reward : 9.0 , loss : 0.24288334223054164\n",
      "1461 time steps done,  119 episodes done. Reward : 11.0 , loss : 5.747164559362475\n",
      "1473 time steps done,  120 episodes done. Reward : 12.0 , loss : 0.39635627454056477\n",
      "1482 time steps done,  121 episodes done. Reward : 9.0 , loss : 0.7172479602674638\n",
      "1490 time steps done,  122 episodes done. Reward : 8.0 , loss : 0.29504849862094723\n",
      "1498 time steps done,  123 episodes done. Reward : 8.0 , loss : 0.1203767331688519\n",
      "1508 time steps done,  124 episodes done. Reward : 10.0 , loss : 0.3515319364973196\n",
      "1516 time steps done,  125 episodes done. Reward : 8.0 , loss : 0.18742099593340175\n",
      "1524 time steps done,  126 episodes done. Reward : 8.0 , loss : 0.19060068705614547\n",
      "1535 time steps done,  127 episodes done. Reward : 11.0 , loss : 0.19302840587317185\n",
      "1547 time steps done,  128 episodes done. Reward : 12.0 , loss : 0.15980688159118275\n",
      "1556 time steps done,  129 episodes done. Reward : 9.0 , loss : 1.2580327493189265\n",
      "1564 time steps done,  130 episodes done. Reward : 8.0 , loss : 0.1124507988663686\n",
      "1572 time steps done,  131 episodes done. Reward : 8.0 , loss : 0.25949110196450126\n",
      "1581 time steps done,  132 episodes done. Reward : 9.0 , loss : 0.11053588937871929\n",
      "1592 time steps done,  133 episodes done. Reward : 11.0 , loss : 0.18145450602359728\n",
      "1602 time steps done,  134 episodes done. Reward : 10.0 , loss : 0.04486056850623903\n",
      "1610 time steps done,  135 episodes done. Reward : 8.0 , loss : 0.1190800525079953\n",
      "1620 time steps done,  136 episodes done. Reward : 10.0 , loss : 0.2368855243871831\n",
      "1629 time steps done,  137 episodes done. Reward : 9.0 , loss : 1.3355325137933298\n",
      "1638 time steps done,  138 episodes done. Reward : 9.0 , loss : 4.777768782068872\n",
      "1646 time steps done,  139 episodes done. Reward : 8.0 , loss : 0.6508551873746068\n",
      "1656 time steps done,  140 episodes done. Reward : 10.0 , loss : 0.3560484904492167\n",
      "1666 time steps done,  141 episodes done. Reward : 10.0 , loss : 0.9851141223088264\n",
      "1676 time steps done,  142 episodes done. Reward : 10.0 , loss : 0.17842583869049078\n",
      "1687 time steps done,  143 episodes done. Reward : 11.0 , loss : 0.2942272827909623\n",
      "1697 time steps done,  144 episodes done. Reward : 10.0 , loss : 0.15603985691597658\n",
      "1707 time steps done,  145 episodes done. Reward : 10.0 , loss : 0.8120053458413183\n",
      "1718 time steps done,  146 episodes done. Reward : 11.0 , loss : 0.3609036015639375\n",
      "1728 time steps done,  147 episodes done. Reward : 10.0 , loss : 0.8009314658164911\n",
      "1739 time steps done,  148 episodes done. Reward : 11.0 , loss : 0.2954553335572876\n",
      "1750 time steps done,  149 episodes done. Reward : 11.0 , loss : 2.1854532723506206\n",
      "1761 time steps done,  150 episodes done. Reward : 11.0 , loss : 0.44011877146648887\n",
      "1773 time steps done,  151 episodes done. Reward : 12.0 , loss : 0.9545474847426302\n",
      "1784 time steps done,  152 episodes done. Reward : 11.0 , loss : 0.12726021925046935\n",
      "1792 time steps done,  153 episodes done. Reward : 8.0 , loss : 0.06826647769810944\n",
      "1801 time steps done,  154 episodes done. Reward : 9.0 , loss : 2.833735154310716\n",
      "1812 time steps done,  155 episodes done. Reward : 11.0 , loss : 0.1432544672062229\n",
      "1821 time steps done,  156 episodes done. Reward : 9.0 , loss : 0.20069500644313285\n",
      "1830 time steps done,  157 episodes done. Reward : 9.0 , loss : 1.8854947667546833\n",
      "1844 time steps done,  158 episodes done. Reward : 14.0 , loss : 0.6344603384499833\n",
      "1853 time steps done,  159 episodes done. Reward : 9.0 , loss : 0.3728367042136447\n",
      "1864 time steps done,  160 episodes done. Reward : 11.0 , loss : 0.10800876631505651\n",
      "1874 time steps done,  161 episodes done. Reward : 10.0 , loss : 0.8435550399417779\n",
      "1886 time steps done,  162 episodes done. Reward : 12.0 , loss : 0.14497359962667067\n",
      "1896 time steps done,  163 episodes done. Reward : 10.0 , loss : 1.0332416496918726\n",
      "1906 time steps done,  164 episodes done. Reward : 10.0 , loss : 0.22702893278956882\n",
      "1916 time steps done,  165 episodes done. Reward : 10.0 , loss : 1.1267410711680659\n",
      "1925 time steps done,  166 episodes done. Reward : 9.0 , loss : 0.36215960792672924\n",
      "1934 time steps done,  167 episodes done. Reward : 9.0 , loss : 0.20642842673946654\n",
      "1943 time steps done,  168 episodes done. Reward : 9.0 , loss : 0.45404316177188536\n",
      "1953 time steps done,  169 episodes done. Reward : 10.0 , loss : 0.19010880888102555\n",
      "1962 time steps done,  170 episodes done. Reward : 9.0 , loss : 0.2130681992346078\n",
      "1970 time steps done,  171 episodes done. Reward : 8.0 , loss : 0.18180901905055172\n",
      "1978 time steps done,  172 episodes done. Reward : 8.0 , loss : 0.24794035294158093\n",
      "1988 time steps done,  173 episodes done. Reward : 10.0 , loss : 0.13490664157059143\n",
      "1997 time steps done,  174 episodes done. Reward : 9.0 , loss : 0.3322229663300879\n",
      "2006 time steps done,  175 episodes done. Reward : 9.0 , loss : 0.24384965347391538\n",
      "2016 time steps done,  176 episodes done. Reward : 10.0 , loss : 0.2770577067283034\n",
      "2026 time steps done,  177 episodes done. Reward : 10.0 , loss : 2.036804622564763\n",
      "2035 time steps done,  178 episodes done. Reward : 9.0 , loss : 0.35536623932908534\n",
      "2045 time steps done,  179 episodes done. Reward : 10.0 , loss : 0.19089699314398267\n",
      "2055 time steps done,  180 episodes done. Reward : 10.0 , loss : 1.1382374962892208\n",
      "2068 time steps done,  181 episodes done. Reward : 13.0 , loss : 0.3202081250381122\n",
      "2080 time steps done,  182 episodes done. Reward : 12.0 , loss : 1.2910884844138544\n",
      "2090 time steps done,  183 episodes done. Reward : 10.0 , loss : 0.2522356555644342\n",
      "2101 time steps done,  184 episodes done. Reward : 11.0 , loss : 0.5152408562705689\n",
      "2113 time steps done,  185 episodes done. Reward : 12.0 , loss : 0.10378873223418239\n",
      "2126 time steps done,  186 episodes done. Reward : 13.0 , loss : 0.2040287663590416\n",
      "2137 time steps done,  187 episodes done. Reward : 11.0 , loss : 0.22484404745772063\n",
      "2146 time steps done,  188 episodes done. Reward : 9.0 , loss : 0.9698890410599578\n",
      "2155 time steps done,  189 episodes done. Reward : 9.0 , loss : 0.4289042142913816\n",
      "2170 time steps done,  190 episodes done. Reward : 15.0 , loss : 0.24488747651763046\n",
      "2185 time steps done,  191 episodes done. Reward : 15.0 , loss : 0.29938377449771614\n",
      "2194 time steps done,  192 episodes done. Reward : 9.0 , loss : 0.5530286344683607\n",
      "2205 time steps done,  193 episodes done. Reward : 11.0 , loss : 0.266082978241794\n",
      "2214 time steps done,  194 episodes done. Reward : 9.0 , loss : 0.202760154807321\n",
      "2224 time steps done,  195 episodes done. Reward : 10.0 , loss : 0.24150615505807213\n",
      "2234 time steps done,  196 episodes done. Reward : 10.0 , loss : 0.0636439418639093\n",
      "2244 time steps done,  197 episodes done. Reward : 10.0 , loss : 0.2864660780352898\n",
      "2253 time steps done,  198 episodes done. Reward : 9.0 , loss : 0.6241043964158272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2261 time steps done,  199 episodes done. Reward : 8.0 , loss : 0.44087704880124146\n",
      "2271 time steps done,  200 episodes done. Reward : 10.0 , loss : 0.21854245516340925\n",
      "2281 time steps done,  201 episodes done. Reward : 10.0 , loss : 0.19851250107693066\n",
      "2291 time steps done,  202 episodes done. Reward : 10.0 , loss : 0.3973322223123934\n",
      "2301 time steps done,  203 episodes done. Reward : 10.0 , loss : 0.14704834722861557\n",
      "2313 time steps done,  204 episodes done. Reward : 12.0 , loss : 0.2380701032636433\n",
      "2327 time steps done,  205 episodes done. Reward : 14.0 , loss : 0.1783171448518998\n",
      "2337 time steps done,  206 episodes done. Reward : 10.0 , loss : 0.2846959623331189\n",
      "2352 time steps done,  207 episodes done. Reward : 15.0 , loss : 0.25604893915835636\n",
      "2363 time steps done,  208 episodes done. Reward : 11.0 , loss : 0.5211400015641826\n",
      "2373 time steps done,  209 episodes done. Reward : 10.0 , loss : 0.24840317184754493\n",
      "2385 time steps done,  210 episodes done. Reward : 12.0 , loss : 0.2993307017269779\n",
      "2395 time steps done,  211 episodes done. Reward : 10.0 , loss : 0.46100630953618166\n",
      "2405 time steps done,  212 episodes done. Reward : 10.0 , loss : 0.13501804154146646\n",
      "2417 time steps done,  213 episodes done. Reward : 12.0 , loss : 0.16279874061871727\n",
      "2429 time steps done,  214 episodes done. Reward : 12.0 , loss : 0.14146392014166564\n",
      "2441 time steps done,  215 episodes done. Reward : 12.0 , loss : 0.5257260456860515\n",
      "2450 time steps done,  216 episodes done. Reward : 9.0 , loss : 0.5853328654636377\n",
      "2458 time steps done,  217 episodes done. Reward : 8.0 , loss : 0.038848022313727734\n",
      "2468 time steps done,  218 episodes done. Reward : 10.0 , loss : 0.8519065481570823\n",
      "2479 time steps done,  219 episodes done. Reward : 11.0 , loss : 0.32582683906519677\n",
      "2491 time steps done,  220 episodes done. Reward : 12.0 , loss : 2.324550288829614\n",
      "2500 time steps done,  221 episodes done. Reward : 9.0 , loss : 0.09252457335133518\n",
      "2510 time steps done,  222 episodes done. Reward : 10.0 , loss : 0.17099140244150704\n",
      "2520 time steps done,  223 episodes done. Reward : 10.0 , loss : 0.23329534267240082\n",
      "2529 time steps done,  224 episodes done. Reward : 9.0 , loss : 0.23813954541422822\n",
      "2537 time steps done,  225 episodes done. Reward : 8.0 , loss : 0.09749269631016327\n",
      "2550 time steps done,  226 episodes done. Reward : 13.0 , loss : 0.29035056513493535\n",
      "2562 time steps done,  227 episodes done. Reward : 12.0 , loss : 0.697686560240857\n",
      "2572 time steps done,  228 episodes done. Reward : 10.0 , loss : 0.7215598053164909\n",
      "2583 time steps done,  229 episodes done. Reward : 11.0 , loss : 0.6589097619249616\n",
      "2591 time steps done,  230 episodes done. Reward : 8.0 , loss : 0.17937285088825722\n",
      "2603 time steps done,  231 episodes done. Reward : 12.0 , loss : 0.11757241575424027\n",
      "2616 time steps done,  232 episodes done. Reward : 13.0 , loss : 0.08692171438935152\n",
      "2632 time steps done,  233 episodes done. Reward : 16.0 , loss : 1.3225261605433567\n",
      "2640 time steps done,  234 episodes done. Reward : 8.0 , loss : 0.20065359502340155\n",
      "2651 time steps done,  235 episodes done. Reward : 11.0 , loss : 0.2183408561584359\n",
      "2667 time steps done,  236 episodes done. Reward : 16.0 , loss : 0.165243656392948\n",
      "2677 time steps done,  237 episodes done. Reward : 10.0 , loss : 0.4806373478348439\n",
      "2688 time steps done,  238 episodes done. Reward : 11.0 , loss : 0.036480868896505175\n",
      "2696 time steps done,  239 episodes done. Reward : 8.0 , loss : 0.6171566918466831\n",
      "2708 time steps done,  240 episodes done. Reward : 12.0 , loss : 0.2879887662451715\n",
      "2724 time steps done,  241 episodes done. Reward : 16.0 , loss : 0.7494186001462703\n",
      "2735 time steps done,  242 episodes done. Reward : 11.0 , loss : 0.12602931127311887\n",
      "2749 time steps done,  243 episodes done. Reward : 14.0 , loss : 0.3256560681420437\n",
      "2766 time steps done,  244 episodes done. Reward : 17.0 , loss : 0.08939484181785766\n",
      "2775 time steps done,  245 episodes done. Reward : 9.0 , loss : 0.11621787227909712\n",
      "2783 time steps done,  246 episodes done. Reward : 8.0 , loss : 0.8141264192525733\n",
      "2792 time steps done,  247 episodes done. Reward : 9.0 , loss : 0.45159084495280233\n",
      "2807 time steps done,  248 episodes done. Reward : 15.0 , loss : 0.7813756615400429\n",
      "2821 time steps done,  249 episodes done. Reward : 14.0 , loss : 0.12575952877974944\n",
      "2832 time steps done,  250 episodes done. Reward : 11.0 , loss : 0.26280805823883374\n",
      "2847 time steps done,  251 episodes done. Reward : 15.0 , loss : 0.22487650233746723\n",
      "2856 time steps done,  252 episodes done. Reward : 9.0 , loss : 0.09326399208354808\n",
      "2867 time steps done,  253 episodes done. Reward : 11.0 , loss : 0.16627419821870898\n",
      "2875 time steps done,  254 episodes done. Reward : 8.0 , loss : 0.29764588993476004\n",
      "2883 time steps done,  255 episodes done. Reward : 8.0 , loss : 0.18358593013254\n",
      "2894 time steps done,  256 episodes done. Reward : 11.0 , loss : 0.47409420717711853\n",
      "2904 time steps done,  257 episodes done. Reward : 10.0 , loss : 0.41687226431991603\n",
      "2914 time steps done,  258 episodes done. Reward : 10.0 , loss : 1.7021722540578077\n",
      "2929 time steps done,  259 episodes done. Reward : 15.0 , loss : 0.07999897820838267\n",
      "2944 time steps done,  260 episodes done. Reward : 15.0 , loss : 0.5422616115431388\n",
      "2955 time steps done,  261 episodes done. Reward : 11.0 , loss : 0.4588271636907808\n",
      "2963 time steps done,  262 episodes done. Reward : 8.0 , loss : 0.14842424572139717\n",
      "2974 time steps done,  263 episodes done. Reward : 11.0 , loss : 0.30764732376502774\n",
      "2983 time steps done,  264 episodes done. Reward : 9.0 , loss : 0.7990103527399224\n",
      "2994 time steps done,  265 episodes done. Reward : 11.0 , loss : 0.18825768708635907\n",
      "3005 time steps done,  266 episodes done. Reward : 11.0 , loss : 0.09721227446360342\n",
      "Total training time : 853.3419842720032 Actual training time : 841.5187776088715\n",
      "Win ratio (nb of games won/nb of games played) : 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Training for ',max_time_steps,'time-steps ...')\n",
    "\n",
    "while time_step<max_time_steps:\n",
    "    loss = 0.\n",
    "    acc_reward = 0\n",
    "    C=0\n",
    "    e+=1\n",
    "    input_t = env.reset()\n",
    "    input_t = input_t.reshape((1,observation_shape))\n",
    "    game_over = False\n",
    "    \n",
    "    while not game_over:\n",
    "\n",
    "        input_tm1 = input_t.astype('float32')\n",
    "        \n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(0, num_actions, size=1)[0]\n",
    "        else:\n",
    "            q = predict_NN(input_tm1, W1, b1, Wo, bo)\n",
    "            action = np.argmax(q)\n",
    "\n",
    "        input_t, reward, game_over, infodemerde = env.step(action)\n",
    "        input_t = input_t.reshape((1,observation_shape))\n",
    "        \n",
    "        acc_reward += reward\n",
    "\n",
    "        agent.remind([input_tm1, action, reward, input_t], game_over)\n",
    "\n",
    "        inputs, targets = agent.get_batch(W1, b1, Wo, bo, batch_size=batch_size)\n",
    "        inputs=inputs.astype('float32')\n",
    "        targets=targets.astype('float32')\n",
    "\n",
    "        t2 = time.time()  #start of actual training time\n",
    "\n",
    "        #TRAIN\n",
    "        err, W1, b1, Wo, bo = train_on_batch(inputs, targets, classifier, x=x, y=y_g, index=index, learning_rate=learning_rate, batch_size=len(inputs))\n",
    "\n",
    "        t3 = time.time() #end of actual training time\n",
    "        actual_total += t3-t2\n",
    "\n",
    "        time_step += 1\n",
    "\n",
    "        if acc_reward>=200:\n",
    "            game_over=True\n",
    "            win_cnt+=1\n",
    "\n",
    "    print(time_step,'time steps done, ',e,'episodes done. Reward :', acc_reward, ', loss :', err[0])\n",
    "\n",
    "t1 = time.time() #end of training time\n",
    "total = t1-t0\n",
    "print('Total training time :', total,'Actual training time :', actual_total)\n",
    "print('Win ratio (nb of games won/nb of games played) :', win_cnt/e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for  10 episodes ...\n",
      "0 episodes done. Reward : 8.0\n",
      "1 episodes done. Reward : 8.0\n",
      "2 episodes done. Reward : 10.0\n",
      "3 episodes done. Reward : 9.0\n",
      "4 episodes done. Reward : 10.0\n",
      "5 episodes done. Reward : 9.0\n",
      "6 episodes done. Reward : 10.0\n",
      "7 episodes done. Reward : 9.0\n",
      "8 episodes done. Reward : 10.0\n",
      "9 episodes done. Reward : 8.0\n",
      "The average reward over the test was : 9.1\n"
     ]
    }
   ],
   "source": [
    "#nb of episodes to test\n",
    "nb_e_test=10\n",
    "\n",
    "#Total reward over the episodes\n",
    "total_rew=0\n",
    "\n",
    "print('Testing for ',nb_e_test,'episodes ...')\n",
    "\n",
    "for episode in range(nb_e_test):\n",
    "    acc_reward = 0\n",
    "    input_t = env.reset()\n",
    "    input_t = input_t.reshape((1,observation_shape))\n",
    "    game_over = False\n",
    "\n",
    "    while not game_over:\n",
    "        input_tm1 = input_t.astype('float32')\n",
    "        \n",
    "        q = predict_NN(input_tm1, W1, b1, Wo, bo)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        input_t, reward, game_over, infodemerde = env.step(action)\n",
    "        input_t = input_t.reshape((1,observation_shape))\n",
    "        \n",
    "        acc_reward += reward\n",
    "\n",
    "        if acc_reward>=200:\n",
    "            game_over=True\n",
    "            \n",
    "    total_rew+=acc_reward\n",
    "\n",
    "    print(episode,'episodes done. Reward :', acc_reward)\n",
    "\n",
    "\n",
    "print('The average reward over the test was :',total_rew/nb_e_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import theano\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import math\n",
    "import json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "from keras.models import model_from_json\n",
    "import time\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_Keras(object):\n",
    "     \n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        \"\"\"Define max length of memory and gamma\"\"\"\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remind(self, states, game_over):      \n",
    "        \"\"\"Add experience to memory\"\"\"\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:         #Delete the first experience if the memory is too long\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, target_model, model, batch_size=10):\n",
    "        \"\"\"Get the batch input and targets we will train on\"\"\"\n",
    "        len_memory = len(self.memory)           #length of memory vector\n",
    "        num_actions = model.output_shape[-1]    #number of actions in action space\n",
    "        \n",
    "                                                #states is an experience : [input_t_minus_1, action, reward, input_t],\n",
    "        env_dim = self.memory[0][0][0].shape[1] #so memory[0] is state and memory[0][0][0].shape[1] is the size of the input\n",
    "        \n",
    "                                                \n",
    "        inputs = np.zeros((min(len_memory, batch_size),#if batch_size<len_memory (it is mostly the case), \n",
    "                           env_dim))                   #then input is a matrix with batch_size rows and size of obs columns\n",
    "        \n",
    "                                                          #Targets is a matrix with batch_size rows and \n",
    "        targets = np.zeros((inputs.shape[0], num_actions))#number of actions columns\n",
    "        \n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,\n",
    "                                                  size=inputs.shape[0])):\n",
    "            \n",
    "            #get experience number idx, idx being a random number in [0,length of memory]\n",
    "            #There are batch_size experiences that are drawn\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            \n",
    "            game_over = self.memory[idx][1]     #Is the game over ? if done in gym\n",
    "\n",
    "            inputs[i:i+1] = state_t             #The inputs of the NN are the state of the experience drawn\n",
    "            \n",
    "                                                          # target_model.predict(state_t)[0] is the \n",
    "            targets[i] = target_model.predict(state_t)[0] #vector of Q(state_t) for each action \n",
    "            \n",
    "                                                #Q_sa=Q_target(s,argmax_a'{Q(s',a')}\n",
    "                                                #index is the action you that maximizes the Q-value of the current network\n",
    "            index, maxima = max(enumerate(model.predict(state_tp1)[0]), key=operator.itemgetter(1))\n",
    "                                                            #We take the value of the target\n",
    "            Q_sa = target_model.predict(state_tp1)[0][index]#network for action index\n",
    "            \n",
    "            if game_over:  # if game_over is True\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                                                                       # the target for this particular experience is : \n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa #reward_t + gamma * max_a' Q(s', a')\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Environment and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')  \n",
    "\n",
    "learning_rate=0.001             #learning rate\n",
    "epsilon = .1                    #exploration parameter\n",
    "num_actions = env.action_space.n#Number of possible actions\n",
    "max_memory = 4000000000         #Length of memory\n",
    "hidden_size = 200               #Number of hidden units\n",
    "batch_size = 50                 #Size of batch for training\n",
    "acc_reward=0                    #Accumulated reward over epoch\n",
    "time_step=0                     #counter of time-steps\n",
    "max_time_steps=3000             #total number of time-steps to train on\n",
    "everyC=5                        #Number of times we update the target network\n",
    "C=0                             #Parameter C\n",
    "\n",
    "#shape of observations\n",
    "observation_shape = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adam=keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, clipvalue=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Current DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(hidden_size, input_dim=observation_shape, activation='relu')) #first fully connected layer, activation RELU\n",
    "model.add(Dense(num_actions))                                         #last fully connected layer, output Q(s,a,theta)\n",
    "\n",
    "model.compile(optimizer=Adam, loss='mean_squared_error')              #choose optimization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Target DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = Sequential()\n",
    "target_model.add(Dense(hidden_size, input_dim=observation_shape, activation='relu'))#first fully connected layer, activation RELU\n",
    "target_model.add(Dense(num_actions))                                        #last fully connected layer, output Q(s,a,theta)\n",
    "target_model.compile(optimizer=Adam, loss='mean_squared_error')             #choose optimization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Initializing the Experience Replay object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters : epsilon : 0.1 C : 5 , learning rate : 0.001 batch size for training : 50\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 200)               1000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 1,402\n",
      "Trainable params: 1,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for  2000 time-steps ...\n"
     ]
    }
   ],
   "source": [
    "agent = Agent_Keras(max_memory=max_memory)\n",
    "\n",
    "win_cnt = 0      #nb of games won\n",
    "t0 = time.time() #start of traning time\n",
    "actual_total=0   #actual training time\n",
    "e=0              #nb of episodes\n",
    "\n",
    "print('Parameters :','epsilon :', epsilon,'C :', everyC,', learning rate :', learning_rate, 'batch size for training :', batch_size)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print('Training for ',max_time_steps,'time-steps ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 time steps done,  1 episodes done. Reward : 9.0 , loss : 4.482597440481186\n",
      "19 time steps done,  2 episodes done. Reward : 10.0 , loss : 4.7649412751197815\n",
      "29 time steps done,  3 episodes done. Reward : 10.0 , loss : 5.2484195828437805\n",
      "38 time steps done,  4 episodes done. Reward : 9.0 , loss : 5.54288786649704\n",
      "48 time steps done,  5 episodes done. Reward : 10.0 , loss : 8.060454785823822\n",
      "56 time steps done,  6 episodes done. Reward : 8.0 , loss : 8.423350751399994\n",
      "66 time steps done,  7 episodes done. Reward : 10.0 , loss : 13.036950409412384\n",
      "74 time steps done,  8 episodes done. Reward : 8.0 , loss : 12.72737991809845\n",
      "82 time steps done,  9 episodes done. Reward : 8.0 , loss : 13.02591872215271\n",
      "92 time steps done,  10 episodes done. Reward : 10.0 , loss : 19.00445032119751\n",
      "100 time steps done,  11 episodes done. Reward : 8.0 , loss : 16.702967643737793\n",
      "109 time steps done,  12 episodes done. Reward : 9.0 , loss : 13.76010286808014\n",
      "119 time steps done,  13 episodes done. Reward : 10.0 , loss : 15.613923013210297\n",
      "128 time steps done,  14 episodes done. Reward : 9.0 , loss : 11.612555742263794\n",
      "140 time steps done,  15 episodes done. Reward : 12.0 , loss : 13.80399239063263\n",
      "150 time steps done,  16 episodes done. Reward : 10.0 , loss : 11.676367938518524\n",
      "161 time steps done,  17 episodes done. Reward : 11.0 , loss : 10.97869348526001\n",
      "169 time steps done,  18 episodes done. Reward : 8.0 , loss : 5.992902725934982\n",
      "178 time steps done,  19 episodes done. Reward : 9.0 , loss : 6.779929876327515\n",
      "190 time steps done,  20 episodes done. Reward : 12.0 , loss : 6.714167922735214\n",
      "199 time steps done,  21 episodes done. Reward : 9.0 , loss : 4.8947072476148605\n",
      "209 time steps done,  22 episodes done. Reward : 10.0 , loss : 4.1318575739860535\n",
      "219 time steps done,  23 episodes done. Reward : 10.0 , loss : 4.481951117515564\n",
      "233 time steps done,  24 episodes done. Reward : 14.0 , loss : 5.553201250731945\n",
      "245 time steps done,  25 episodes done. Reward : 12.0 , loss : 4.711343631148338\n",
      "253 time steps done,  26 episodes done. Reward : 8.0 , loss : 2.5684525221586227\n",
      "262 time steps done,  27 episodes done. Reward : 9.0 , loss : 2.189805507659912\n",
      "272 time steps done,  28 episodes done. Reward : 10.0 , loss : 2.9614876806735992\n",
      "283 time steps done,  29 episodes done. Reward : 11.0 , loss : 3.064222440123558\n",
      "293 time steps done,  30 episodes done. Reward : 10.0 , loss : 2.37132116407156\n",
      "303 time steps done,  31 episodes done. Reward : 10.0 , loss : 1.7518587931990623\n",
      "314 time steps done,  32 episodes done. Reward : 11.0 , loss : 2.2286572381854057\n",
      "323 time steps done,  33 episodes done. Reward : 9.0 , loss : 1.9769665896892548\n",
      "333 time steps done,  34 episodes done. Reward : 10.0 , loss : 2.3744156025350094\n",
      "343 time steps done,  35 episodes done. Reward : 10.0 , loss : 1.6991040259599686\n",
      "353 time steps done,  36 episodes done. Reward : 10.0 , loss : 1.279002457857132\n",
      "363 time steps done,  37 episodes done. Reward : 10.0 , loss : 1.505371369421482\n",
      "374 time steps done,  38 episodes done. Reward : 11.0 , loss : 1.2555598244071007\n",
      "386 time steps done,  39 episodes done. Reward : 12.0 , loss : 1.8458587899804115\n",
      "396 time steps done,  40 episodes done. Reward : 10.0 , loss : 1.216722372919321\n",
      "405 time steps done,  41 episodes done. Reward : 9.0 , loss : 1.1682087071239948\n",
      "414 time steps done,  42 episodes done. Reward : 9.0 , loss : 1.0962669178843498\n",
      "423 time steps done,  43 episodes done. Reward : 9.0 , loss : 0.8414915539324284\n",
      "432 time steps done,  44 episodes done. Reward : 9.0 , loss : 0.8956848941743374\n",
      "441 time steps done,  45 episodes done. Reward : 9.0 , loss : 0.6680316217243671\n",
      "450 time steps done,  46 episodes done. Reward : 9.0 , loss : 0.6083658300340176\n",
      "459 time steps done,  47 episodes done. Reward : 9.0 , loss : 0.7162412442266941\n",
      "470 time steps done,  48 episodes done. Reward : 11.0 , loss : 0.686209499835968\n",
      "481 time steps done,  49 episodes done. Reward : 11.0 , loss : 0.8892046585679054\n",
      "490 time steps done,  50 episodes done. Reward : 9.0 , loss : 0.5999448299407959\n",
      "501 time steps done,  51 episodes done. Reward : 11.0 , loss : 0.7002150230109692\n",
      "514 time steps done,  52 episodes done. Reward : 13.0 , loss : 0.7643165159970522\n",
      "530 time steps done,  53 episodes done. Reward : 16.0 , loss : 1.104968886822462\n",
      "558 time steps done,  54 episodes done. Reward : 28.0 , loss : 2.1500980481505394\n",
      "595 time steps done,  55 episodes done. Reward : 37.0 , loss : 3.186581442132592\n",
      "636 time steps done,  56 episodes done. Reward : 41.0 , loss : 4.827209111303091\n",
      "665 time steps done,  57 episodes done. Reward : 29.0 , loss : 4.889812307432294\n",
      "694 time steps done,  58 episodes done. Reward : 29.0 , loss : 4.664594914764166\n",
      "718 time steps done,  59 episodes done. Reward : 24.0 , loss : 6.853663377463818\n",
      "762 time steps done,  60 episodes done. Reward : 44.0 , loss : 13.16728731803596\n",
      "784 time steps done,  61 episodes done. Reward : 22.0 , loss : 6.488443236798048\n",
      "811 time steps done,  62 episodes done. Reward : 27.0 , loss : 11.869868332520127\n",
      "839 time steps done,  63 episodes done. Reward : 28.0 , loss : 10.49031987041235\n",
      "872 time steps done,  64 episodes done. Reward : 33.0 , loss : 14.489147387444973\n",
      "923 time steps done,  65 episodes done. Reward : 51.0 , loss : 13.860043324530125\n",
      "978 time steps done,  66 episodes done. Reward : 55.0 , loss : 13.723298110067844\n",
      "1029 time steps done,  67 episodes done. Reward : 51.0 , loss : 16.796609736979008\n",
      "1067 time steps done,  68 episodes done. Reward : 38.0 , loss : 11.687867622822523\n",
      "1095 time steps done,  69 episodes done. Reward : 28.0 , loss : 10.337605517357588\n",
      "1175 time steps done,  70 episodes done. Reward : 80.0 , loss : 23.09804867580533\n",
      "1256 time steps done,  71 episodes done. Reward : 81.0 , loss : 18.584321442991495\n",
      "1352 time steps done,  72 episodes done. Reward : 96.0 , loss : 20.532509537413716\n",
      "1552 time steps done,  73 episodes done. Reward : 200.0 , loss : 51.608056815341115\n",
      "1701 time steps done,  74 episodes done. Reward : 149.0 , loss : 37.95966115780175\n",
      "1894 time steps done,  75 episodes done. Reward : 193.0 , loss : 39.76995231024921\n",
      "2094 time steps done,  76 episodes done. Reward : 200.0 , loss : 42.775212357752025\n",
      "Total training time : 290.8466217517853 Actual training time : 7.058011531829834\n",
      "Win ratio (nb of games won/nb of games played) : 0.02631578947368421\n"
     ]
    }
   ],
   "source": [
    "while time_step<max_time_steps:\n",
    "    loss = 0.       #Set loss to zero\n",
    "    acc_reward = 0  #Set accumulated reward to 0\n",
    "    C=0             #Set C to zero\n",
    "    e+=1            #Add episode\n",
    "    \n",
    "    input_t = env.reset()\n",
    "    input_t = input_t.reshape((1,observation_shape))\n",
    "    \n",
    "    game_over = False #Since it's the beginning of the game, game_over is not True\n",
    "\n",
    "    while not game_over:\n",
    "        \n",
    "        input_tm1 = input_t            #set this state to be the last state\n",
    "        if np.random.rand() <= epsilon:# get next action according to espilon-greedy policy\n",
    "            #exploration\n",
    "            action = np.random.randint(0, num_actions, size=1)[0]\n",
    "        else:\n",
    "            #exploitation\n",
    "            q = model.predict(input_tm1)\n",
    "            action = np.argmax(q[0])\n",
    "\n",
    "        input_t, reward, game_over, infodemerde = env.step(action) #apply action, get rewards and new state\n",
    "        input_t = input_t.reshape((1,observation_shape))\n",
    "        \n",
    "        acc_reward += reward    #Accumulate reward\n",
    "\n",
    "        agent.remind([input_tm1, action, reward, input_t], game_over) # store experience\n",
    "        \n",
    "        #Create new target network every C updates, by cloning the current network\n",
    "        if C%everyC==0:\n",
    "            model.save_weights(\"model_cartpole_TARGET\", overwrite=True)\n",
    "            with open(\"model_cartpole_TARGET.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile) \n",
    "            target_model.load_weights(\"model_cartpole_TARGET\")\n",
    "            \n",
    "\n",
    "        C += 1         #Increment C\n",
    "        # get batch we will train on\n",
    "        inputs, targets = agent.get_batch(target_model, model, batch_size=batch_size)\n",
    "\n",
    "        t2 = time.time() #start of actual training time\n",
    "        loss += model.train_on_batch(inputs, targets)\n",
    "        t3 = time.time() #end of actual training time\n",
    "        actual_total += t3-t2\n",
    "        time_step += 1   #increment time-step\n",
    "\n",
    "        if acc_reward>=200: #end game if max score is reached\n",
    "            game_over=True\n",
    "            win_cnt+=1\n",
    "\n",
    "    print(time_step,'time steps done, ',e,'episodes done. Reward :', acc_reward, ', loss :', loss)\n",
    "\n",
    "t1 = time.time() #end of training time\n",
    "total = t1-t0\n",
    "print('Total training time :', total,'Actual training time :', actual_total)\n",
    "print('Win ratio (nb of games won/nb of games played) :', win_cnt/e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for  10 episodes ...\n",
      "0 episodes done. Reward : 200.0\n",
      "1 episodes done. Reward : 173.0\n",
      "2 episodes done. Reward : 131.0\n",
      "3 episodes done. Reward : 149.0\n",
      "4 episodes done. Reward : 200.0\n",
      "5 episodes done. Reward : 200.0\n",
      "6 episodes done. Reward : 200.0\n",
      "7 episodes done. Reward : 200.0\n",
      "8 episodes done. Reward : 200.0\n",
      "9 episodes done. Reward : 200.0\n",
      "The average reward over the test was : 185.3\n"
     ]
    }
   ],
   "source": [
    "nb_e_test=10  #nb of episodes to test\n",
    "total_rew=0   #Total reward over the episodes\n",
    "\n",
    "print('Testing for ',nb_e_test,'episodes ...')\n",
    "\n",
    "for episode in range(nb_e_test):    #set accumulated reward to 0\n",
    "    acc_reward = 0\n",
    "    \n",
    "    input_t = env.reset()\n",
    "    input_t = input_t.reshape((1,observation_shape))\n",
    "    \n",
    "    game_over = False\n",
    "\n",
    "    while not game_over:\n",
    "        \n",
    "        input_tm1 = input_t\n",
    "\n",
    "        q = model.predict(input_tm1)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        input_t, reward, game_over, infodemerde = env.step(action)\n",
    "        input_t = input_t.reshape((1,observation_shape))\n",
    "\n",
    "        acc_reward += reward\n",
    "\n",
    "        if acc_reward>=200:\n",
    "            game_over=True\n",
    "\n",
    "    total_rew+=acc_reward\n",
    "\n",
    "    print(episode,'episodes done. Reward :', acc_reward)\n",
    "\n",
    "print('The average reward over the test was :',total_rew/nb_e_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results appear on the following Table :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{array}{|c|r|r|}\n",
    "  \\hline\n",
    "\\text{Time for training Network in seconds} & CPU & GPU \\\\ \n",
    "  \\hline\n",
    "Theano & 576 & 540\\\\ \n",
    "   \\hline\n",
    "Keras & 30 & 7\\\\\n",
    "\\hline\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{array}{|c|r|r|}\n",
    "  \\hline\n",
    "\\text{Time for whole algorithm in seconds} & CPU & GPU \\\\ \n",
    "  \\hline\n",
    "Theano & 552 & 585\\\\ \n",
    "   \\hline\n",
    "Keras & 274 & 296\\\\\n",
    "\\hline\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the GPU used is a NVIDIA GeForce 740M. The CPU is an Intel Core i7 (2.4GHz). This was done using 3000 steps of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the only improvement of using a GPU is for the time it takes to train a network. Plus, it not being immediate to switch and tell Theano not to use the GPU (By the way, if you have a GPU or not, the times will surely be different). In any case, Keras does perform better on a GPU, roughly $4 \\times$ faster. GPU is perfect for matrix multiplication and that is where the difference is made. Indeed, back-propagation is done by differentiation of the loss with respect to the weights and the chain rule, and so calculations are indeed made quicker, especially thanks to the Adam optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we notice that Theano's results aren't better using GPU. This could be because of my beginner's skils using Theano, and there are still some parts that could be really improved. Plus, there are a lot of dependent steps in the Theano part, which means using a GPU doesn't have an added value. After reading some literature on the subject, having a lot of 'if' statements also aren't well parallelized, which is unfortunetaly the case in that part of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should note however that the convergence for the Theano part of the code isn't always immediate. Sometimes, the Theano algorithm needs more than 3000 steps to converge, yet sometimes after 1000 steps we already get good scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was fully beneficial for me, since I had never worked with Theano nor Keras before. I've been wanting to see what Reinforcement Learning looked like in detail. In addition, I now have my NVIDIA GPU linked up to Python, which is always nice to have, considering it took some time to configure it to my PC's requirements.\n",
    "\n",
    "Overall, the Q-learning model found is very satisfying according to the score obtained averaging around 200. Whether on Theano or Keras, both scores can be pleasant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
